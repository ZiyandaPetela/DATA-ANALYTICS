# DATA-ANALYTICS
# INTRODUCTION 
Confident computer user?
A confident computer user is one who knows more than just the required key presses to operate the software they are using.
* The two most important qualities of a confident computer user are:
   * Knowing how to cope when things go wrong
   * Knowing how to learn new computing skills independently
* Personal computing is changing so fast and becoming so complex that it never gets a chance to settle down and become really reliable.
* There are various ways of coping with computing problems:
   * Save your work often – every few minutes, not every few hours.  
   *  Back up all your important work regularly – if you work on your computer all day this should be done daily. This means making copies of files somewhere else.
   * Look for workarounds.  There is usually more than one way of getting what you want to be done

* Learning New Computing Skills
There are several principles for learning new computing skills:
Learning different computer skills and how to use different computer programs is actually fairly easy.  There are numerous websites and YouTube videos to show how. Some of the principles for learning new computing skills are to try things out and explore. 
Do not be afraid to try things out and explore. You almost certainly will not do any harm by making mistakes – modern software will usually let you undo errors and will warn you before performing potentially damaging actions. 
Look for patterns. Find the differences and similarities between the ways in which things are done in different software, so you can begin to guess what to expect.
You will learn a lot by helping other people in addition to getting your own problems solved by them. During this program, you will be able to develop your support network further by using the Internet. E-mail, chat, and the Web all greatly increase the ways in which you can seek help and information and, of course, help others.
# Data Analytics
Analytics is at the heart of modern business. Virtually every organization collects large quantities of data about its customers, products, employees, and service offerings. Managers naturally seek to analyze that data and harness the information it contains to improve the efficiency, effectiveness, and profitability of their work.
The ultimate role of a data analyst is to transform raw data into actionable insights that guide decision-making processes within an organization. This involves several key responsibilities and skills.

1. Data Collection and Preparation:

Sourcing data from various channels, including databases, spreadsheets, and external sources,
Cleaning and organizing the data to ensure it is accurate, consistent, and ready for analysis.
2. Data Analysis:

Employing statistical methods, machine learning techniques, or other analytic tools to interpret data,
Identifying trends, patterns, and correlations that might not be immediately obvious.
3. Data Visualization and Storytelling:

Creating visual representations of the data, such as charts, graphs, and dashboards, to make complex information easily understandable,
Articulating findings in a compelling narrative to communicate the significance of the data to stakeholders.
4. Decision Support:

Making recommendations based on data-driven insights to help guide business decisions,
Providing context around the data, including potential implications and future trends.
5. Collaboration and Communication:
Working closely with other departments, such as marketing, finance, and operations, to understand their data needs and provide insights,
Effectively communicating complex data findings in a clear and concise manner to non-technical stakeholders,
6. Continuous Learning and Adaptation:
Keeping up-to-date with the latest industry trends, tools, and technologies in data analysis.
Adapting to new types of data and analytical methods as the organization's needs evolve.


# World of Analytics
*  three major pillars that have come together at this moment to allow analytics programs to thrive: data, storage, and computing power.
*  Data -From the organized tables of spreadsheets to the storage of photos, video, and audio recordings, modern businesses create an almost overwhelming avalanche of data that is ripe for use in analytics programs.
*  Storage - The second key trend driving the growth of analytics programs is the increased availability of storage at rapidly decreasing costs.
*  Computing Power
#   Careers in Analytics
* Data analysts and scientists
* AI and machine learning (ML) specialists
* Big Data specialists
Digital marketing and strategy specialists
Process automation specialists
Business development professionals
Digital transformation specialists
Information security analyst
Software and applications developers
Internet of Things (IoT) specialists

# The Analytics Process
* Analysts working with data move through a series of different steps as they seek to gain business value from their organization's data
* Data Acquisition
* Cleaning and Manipulation
* Analysis
* Visualization
* Reporting and Communication
# The Analytics Process is Iterative
* The steps of the analytics process as a series of sequential actions, it is more accurate to think of them as a set of interrelated actions that may be revisited frequently while working with a dataset.
* This process is meant to help you understand the different activities that take place during a data analysis effort and the approximate order in which they typically occur.

# Analytics Techniques
* Descriptive Analytics
* Predictive Analytics
* Predictive Analytics
 # Machine Learning, Artificial Intelligence, and Deep Learning
 * Machine Learning, Artificial Intelligence, and Deep Learning
 * Artificial intelligence (AI) includes any type of technique where you are attempting to get a computer system to imitate human behavior.
 * Machine learning (ML) is a subset of AI techniques. ML techniques attempt to apply statistics to data problems in an effort to discover new knowledge.
 * Deep learning is a further subdivision of machine learning that uses quite complex techniques, known as neural networks, to discover knowledge in a particular way.

# Data Governance

* Data governance programs ensure that the organization has high-quality data and is able to effectively control that data.
# Analytics Tools
* Software helps analysts work through each one of the phases of the analytics process.
*  These tools automate much of the heavy lifting of data analysis, improving the analyst's ability to acquire, clean, manipulate, visualize, and analyze data.
*  They also provide invaluable assistance in reporting and communicating results.
# Understanding Data
# Exploring Data Types
* To understand data types, it is best first to understand data elements. A data element is an attribute about a person, place, or thing containing data within a range of values.
* Data elements also describe characteristics of activities, including orders, transactions, and events.
*  A data type limits the values a data element can have.
*  Tabular data is data organized into a table, made up of columns and rows. A table represents information about a single topic.
*  Each column represents a uniquely named field within a table, also called a variable, about a single characteristic.
*  Spreadsheets, including Microsoft Excel, Google Sheets, and Apple Numbers, are practical tools for representing tabular data.
*   A relational database management system (RDMS), commonly called a database, extends the tabular model. Instead of having all data in a single table, a database organizes related data across multiple tables.

# Structured Data Types
* Structured data is tabular in nature and organized into rows and columns.
* Structured data is what typically comes to mind when looking at a spreadsheet. With clearly defined column headings, spreadsheets are easy to work with and understand.
*  In a spreadsheet, cells are where columns and rows intersect.
# Character
* The character data type limits data entry to only valid characters.
* Characters can include the alphabet that you might see on your keyboard, as well as numbers. *Depending on your needs, multiple data types are available that can enforce character limits.
* alphanumeric is the most widely used data type for storing character-based data. As the name implies, alphanumeric is appropriate when a data element consists of both numbers and letters.

# Character Sets
* Numeric - When numbers exclusively make up values for a data attribute, numeric becomes the data type of choice. 
* Whole Numbers - The integer, and all its subtypes, are for storing whole numbers. 
* Rational Numbers - the numeric data type is for rational numbers that include a decimal point. 
* Date and Time - road category of date, day of year and time of day are data elements that appear with great frequency.
* Currency -financial data is numeric, people prefer seeing the numbers displayed as a specific currency. For example, consider the Number, Dollar, and Euro

# Strong And Weak Typing
* Strong typing is when technology rigidly enforces data types.
* Weak typing loosely enforces data types. Spreadsheets use weak typing to help make it easier for people to accomplish their work.
* Software that uses weak typing can be helpful.
  
# Unstructured Data Types
* Unstructured data is any type of data that does not fit neatly into the tabular model.
* Examples of unstructured data include digital images, audio recordings, video recordings, and open-ended survey responses.
*  Analyzing unstructured data creates a wealth of information and insight.
*  unstructured data type :
*  Binary
*  Audio
*  Images
*  Video
*  Large Text
# Categories of Data
* Quantitative vs. Qualitative Data - Quantitative data consists of numeric values. Data elements whose values come from counting or measuring are quantitative.Qualitative data consists of frequent text values. Data elements whose values describe characteristics, traits, and attitudes are all qualitative. 
* Discrete vs. Continuous Data - discrete data is that it represents measurements that can't be subdivided. You may intuitively think of discrete data as using whole numbers, but that doesn't have to be the case. when you measure things like height and weight, you are collecting continuous data. While whole numbers represent discrete data, continuous data typically need a decimal point. 
* Categorical Data - numeric data, there is categorical data. Text data with a known, finite number of categories is categorical.
* Dimensional Data - Dimensional modeling organizes data into fact tables and dimension tables. Fact tables store measurement data that is of interest to a business.

# Common Data Structures
* Structured Data - Tabular data is structured data, with values stored in a consistent, defined manner, organized into columns and rows.
* Unstructured data is qualitative, describing the characteristics of an event or an object. Images, phrases, audio or video recordings, and descriptive text are all examples of unstructured data. There is very little that is common about different kinds of unstructured data.
* Machine data is a common source of unstructured data. Machine data has various sources, including Internet of Things devices, smartphones, tablets, personal computers, and servers.
* Semi-structured data is data that has structure and that is not tabular. Email is a well-known example of semi-structured data.

# Common File Formats
* Common file formats facilitate data exchange and tool interoperability.
* Text files are one of the most commonly used data file formats. As the name implies, they consist of plain text and are limited in scope to alphanumeric data.
*  One of the reasons text files are so widely adopted is their ability to be opened regardless of platform or operating system without needing a proprietary piece of software.
*  Whether you are using a Microsoft Windows desktop, an Apple MacBook, or a Linux server, you can easily open a text file. Text files are also commonly referred to as flat files.
*  Fixed-Width Files - are more laborious to create since they require a few extra steps.
*  JavaScript Object Notation (JSON) is an open standard file format, designed to add structure to a text file without incurring significant overhead.
*   One of its design principles is that JSON is easily readable by people and easily parsed by modern programming languages.
*    Languages such as Python, R, and Go have libraries containing functions that facilitate reading and writing JSON files.
*   extensible Markup Language (XML) is a markup language that facilitates structuring data in a text file.
*  While conceptually similar to JSON, XML incurs more overhead because it makes extensive use of tags. Tags describe a data element and enclose each value for each data element.
* While these tags help readability, they add a significant amount of overhead.
* HyperText Markup Language (HTML) is a markup language for documents designed to be displayed in a web browser.
*  HTML pages serve as the foundation for how people interact with the World Wide Web. Similar to XML, HTML is a tag-based language.

# CHAPTER THREE - Databases and Data Acquisition
* two categories categories of a database 1.Relational 2.Nonrelational
* Relational databases excel at storing and processing structured data.
* The need to interact with unstructured data is one of the reasons behind the rise of nonrelational databases.
* Relational model builds on the concept of tabular data. In the relational model, an entity contains data for a single subject.
* structure has data points and bring out relationship there
* uses tables to show entities/nouns
* each table has ID to uniquely identify row
* benefits are consistency, stored procedures
* Each of these entities becomes a separate table in the database, with a column for each attribute.
*  Each row represents an instance of the entity.
*   The power of the relational model is that it also allows us to describe how entities connect or relate, to each other.
*   The entity relationship diagram (ERD) is a visual artifact of the data modeling process.
*    It shows the connection between related entities.
* a relationship is a connection between entities. The symbols adjacent to an entity describe the relationship.
# THE RELATIONAL MODEL
* The relational model builds on the concept of tabular data. In the relational model, an entity contains data for a single subject.
* The entity relationship diagram (ERD) is a visual artifact of the data modeling process. It shows the connection between related entities.
* A relationship is a connection between entities. The symbols adjacent to an entity describe the relationship.
* Cardinality refers to the relationship between two entities, showing how many instances of one entity relate to instances in another entity.
* specify cardinality in an ERD with various line endings.
* The first component of the terminator indicates whether the relationship between two entities is optional or required.
* The second component indicates whether an entity instance in the first table is associated with a single entity instance in the related table or if an association can exist with multiple entity instances.
* binary relationship connects two entities,
* ternary relationship connects three entities.

# Relational Databases
* Relational databases are pieces of software that let you make an operational system out of an ERD.
* Relational entities correspond to database tables, and entity attributes correspond to table columns.
* When creating a database table, the ordering of columns does not matter because you can specify the column order when retrieving data from a table.
*  When an attribute becomes a column, you assign it a data type. Completing all of this work results in a diagram known as a schema.
*  To pull data from a relational database table, you perform a query. You compose queries using a programming language called Structured Query Language (SQL).
*   A join uses data values from one table to retrieve associated data in another table, typically using a foreign key.
*  Foreign keys enforce referential integrity, or how consistent the data is in related tables
*   A database administrator (DBA) is a highly trained person who understands how database software interacts with computer hardware.
*   A DBA looks after how the database uses the underlying storage, memory, and processor resources assigned to the database.
*  A DBA also looks for processes that are slowing the entire database down.
*  Microsoft developed SQL Server, and the open source community created offerings including MySQL, MariaDB, and PostgreSQL.
*  Amazon Web Services (AWS) developed Aurora, which is compatible with MySQL and PostgreSQL.
*  Aurora is unique because it takes advantage of AWS's underlying cloud platform and is easy to scale.

# Nonrelational Databses
* A nonrelational database does not have a predefined structure based on tabular data. The result is a highly flexible approach to storing data. However, the data types available in relational databases are absent.
* Examples of nonrelational databases include key-value, document, column family, and graph.

# Database Use Cases
* two categories of data processing: Online Transactional Processing (OLTP) and Online Analytical Processing (OLAP).
* Online Transactional Processing
* OLTP systems handle the transactions we encounter every day.
* OLTP systems balance the ability to write and read data efficiently.
* Normalization
*  is a process for structuring a database in a way that minimizes duplication of data.
* One of the principles is that a given piece of data is stored once and only once. As a result, a normalized database is ideal for processing transactions.
*  First normal form (1NF) is when every row in a table is unique and every column contains a unique value.
* Second normal form (2NF) starts where 1NF leaves off. In addition to each row being unique, 2NF applies an additional rule stating that all nonprimary key values must depend on the entire primary key.
* Third normal form (3NF) builds upon 2NF by adding a rule stating all columns must depend on only the primary key. 

# Online Analytical Processing
* OLAP systems focus on the ability of organizations to analyze data
* While OLAP and OLTP databases can both use relational database technology, their structures are fundamentally different.
* OLTP databases need to balance transactional read and write performance, resulting in a highly normalized design. Typically, OLTP databases are in 3NF.
* Schema Concepts
* The design of a database schema depends on the purpose it serves.
* Transactional systems require highly normalized databases, whereas a denormalized design is more appropriate for analytical systems.
* A data warehouse is a database that aggregates data from many transactional systems for analytical purposes.
*  Transactional data may come from systems that power the human resources, sales, marketing, and product divisions.
*  A data warehouse facilitates analytics across the entire company.
*  A data mart is a subset of a data warehouse.
*   Data warehouses serve the entire organization, whereas data marts focus on the needs of a particular department within the organization.
*  A data lake stores raw data in its native format instead of conforming to a relational database structure.
*  Using a data lake is more complex than a data warehouse or data mart, as it requires additional knowledge about the raw data to make it analytically useful.
*  Relational databases enforce a structure that encapsulates business rules and business logic, both of which are missing in a data lake.
*  The star schema design to facilitate analytical processing gets its name from what the schema looks like when looking at its entity relationship diagram
*  At the centre of the star is a fact table. Fact tables chiefly store numerical facts about a business.
*  the schema design centres on reporting on the cost and profitability of procedures.
*  Qualitative data, including names, addresses, and descriptions, is stored in a series of dimension tables that connect to the main fact table.
*  When data moves from an OLTP design into a star schema, there is a significant amount of data duplication.
*   As such, a star schema consumes more space than its associated OLTP design to store the same data.
*  Another design pattern for data warehousing is the snowflake schema.
*   Snowflake and star schemas are conceptually similar in that they both have a central fact table surrounded by dimensions. \
*   Where the approaches differ is in the handling of dimensions. With a star, the dimension tables connect directly to the fact table.
*    With a snowflake, dimensions have subcategories, which gives the snowflake design its shape.
*    A snowflake schema is less denormalized than the star schema.
*    With a snowflake schema, you may need more than one join to get the data you are looking for.
*  A snowflake schema query is more complex than the equivalent query in a star schema.
*  Data warehouses often use snowflake schemas, since many different systems supply data to the warehouse.
*   Data marts are comparatively less complicated, because they represent a single data subject area. As such, data marts frequently use a star-schema approach. 

* Dimensionality
* Dimensionality refers to the number of attributes a table has. The greater the number of attributes, the higher the dimensionality.
* A dimension table provides additional context around data in fact tables
* Handling Dimensionality
* the start and end date approach - An understanding of this method is required to write a query to retrieve the current price.
* Another method extends the snowflake approach to modelling dimensions. You have a product dimension for the current price and a product history table for maintaining price history.
* One advantage of this approach is that it is easy to retrieve the current price while maintaining access to historical information.
* Another approach is to use an indicator flag - indicator flag method keeps all pricing data in a single place. It also simplifies the query structure to get the current price.

# Data Acquisition Concepts
* Integration
* Data from transactional systems flow into data warehouses and data marts for analysis.
* need to retrieve, reshape, and insert data to move data between operational and analytical environments.
* One approach is known as extract, transform, and load (ETL).
* Extract:  In the first phase, you extract data from the source system and place it in a staging area. The goal of the extract phase is to move data from a relational database into a flat file as quickly as possible.
* Transform:  The second phase transforms the data. The goal is to reformat the data from its transactional structure to the data warehouse's analytical design.
* Load:  The purpose of the load phase is to ensure data gets into the analytical system as quickly as possible.
* Extract, load, and transform (ELT) is a variant of ETL.
*  With ELT, data is extracted from a source database and loaded directly into the data warehouse.
*  Once the extract and load phases are complete, the transformation phase gets underway.
*  One key difference between ETL and ELT is the technical component performing the transformation.
*   With ETL, the data transformation takes place external to a relational database, using a programming language like Python. E
*   ELT uses SQL and the power of a relational database to reformat the data.
*   ELT has an advantage in the speed with which data moves from the operational to the analytical database.


*   ETL Vendors
*   An initial load occurs the first time data is put into a data warehouse.
*   After that initial load, each additional load is a delta load, also known as an incremental load.
*   A delta load only moves changes between systems.

# Data Collection Methods
*Application Programming Interfaces (APIs)- is a structured method for computer systems to exchange information. 
APIs provide a consistent interface to calling applications, regardless of the internal database structure. 
Whoever calls an API has no idea whether a transactional or analytical data store backs it. The internal data structure does not matter as long as the API returns the data you want. 
APIs can be transactional, returning data as JSON objects. 
APIs can also facilitate bulk data extraction, returning CSV files.

* web Services - data is found in private and public data sources and is accessible via a web service.
*  A web service is an API you can call via Hypertext Transfer Protocol (HTTP), the language of the World Wide Web.
* Web Scraping - If data exists in a structured format, you can retrieve it programmatically. Programmatic retrieval of data from a website is
* Human-in-the-Loop - the data you seek exists only in people's minds. 
* Surveys - urveys consist of one question and indicate customer satisfaction
* Survey Tools - Qualtrics is a powerful tool for developing and administering surveys.
* Observation - Observation is the act of collecting primary source data, from either people or machines.
*  Observational data can be qualitative or quantitative.
*   Collecting qualitative observational data leads to unstructured data challenges.
* Sampling - Once you have collected sample data, you can use statistical methods to make generalizations about the entire population

# Working With Data
* To turn a database design into an operational database ready to accept data, use the Data Definition Language (DDL) components of SQL.
*  DDL lets you create, modify, and delete tables and other associated database objects.
*  To generate insights, a productive analyst must be comfortable using the Data Manipulation Language (DML) capabilities of SQL to insert, modify, and retrieve information from databases.
*   While DDL manages the structure of a database, DML manages the data in the database.

*   Data Manipulation

*   Create new data
*   Read existing data
*   Update existing data
*   Delete existing data.

*   Create - INSERT - Creates new data in an existing table
*   Read - SELECT - Retrieves data from an existing table
*   Update - UPDATE- Changes existing data in an existing table
*   Delete- DELETE- Removes existing data from an existing table

*  SQL Considerations
* The keywords in SQL are case-insensitive.
* SQL can also span multiple lines.
* How a query appears is a function of organizational conventions. F
* actors that influence convention include database configuration, query efficiency, and how easy it is for people to read and understand the query.

# Filtering
* Filtering is a way to reduce the data down to only the rows that you need.
* To filter data, you add a WHERE clause to a query

* Filtering and Logical Operators
* A query can have multiple filtering conditions. use a logical operator to account for complex filtering needs.
* Complex queries frequently use multiple logical operators at the same time.
*  It is good to use parentheses around filter conditions to help make queries easy for people to read and understand.
*  Data warehouses often contain millions, billions, or even trillions of individual data records.
*  Filtering data is essential to making effective use of these massive data stores.

# Sorting
* When querying a database, you frequently specify the order in which you want your results to return.
* The ORDER BY clause is the component of a SQL query that makes sorting possible.
* do not have to specify the columns you are using to sort the data in the SELECT clause.
* The ASC keyword at the end of the ORDER BY clause sorts in ascending order whereas using DESC with ORDER BY sorts in descending order.
*  If you are sorting on multiple columns, you can use both ascending and descending as appropriate.
*   Both the ASC and DESC keywords work across various data types, including date, alphanumeric, and numeric.

# Date Functions
* date columns are frequently found in OLAP environments.
*  Date columns also appear in transactional systems. Storing date information about an event facilitates analysis across time.
*  The most important thing to note is that you have to understand the database platform you are using and how that platform handles dates and times
*  Since each platform provider uses different data types for handling this information, you need to familiarize yourself with the functions available from your provider of choice.

# Logical Functions
* Logical functions can make data substitutions when retrieving data.
* Remember that a SELECT statement only retrieves data.
* Another way to generate the output is by using the IFF logical function.
*  The IFF function has the following syntax: IFF(boolean_expression, true_value, false_value)
*  Boolean Expression:  The expression must return either TRUE or FALSE.
*  True Value:  If the Boolean expression returns TRUE, the IFF function will return this value.
*  False Value:  If the Boolean expression returns FALSE, the IFF function will return this value.
*  When using logical functions, you need to balance their convenience with the knowledge that you are replacing data from the database with the function's coded values.
*  The ability to do this type of substitution is a real asset when dividing data into categories.

# Aggregate Functions
* Summarized data helps answer questions that executives have, and aggregate functions are an easy way to summarize data.
* Aggregate functions summarize a query's data and return a single value.
* each database platform supports different aggregation functions
* COUNT -Returns the total number of rows of a query.
* MIN - Returns the minimum value from the results of a query. Note that this works on both alphanumeric and numeric data types.
* MAX - Returns the maximum value from the results of a query. Note that this works on both alphanumeric and numeric data types.
* AVG - Returns the mathematic average of the results of a query.
* SUM -Returns the sum of the results of a query.
* STDDEV- Returns the sample standard deviation of the results of a query.
* can also use aggregate functions to filter data. For example, you may want a query that shows all employees who make less than the average corporate salary.
*  Aggregate functions also operate across subsets of data. For instance, you can calculate total sales by month with a single query.

# System Functions
* Each database platform offers functions that expose data about the database itself.
*  One of the most frequently used system functions returns the current date.
*   The current date is a component of transactional records and enables time-based analysis in the future.
*    The current date is also necessary for a system that uses an effective date approach.
*    System functions also return data about the database environment. 

* Query Optimization
* Parametrization - Whenever a SQL query executes, the database has to parse the query. Parsing translates the human-readable SQL into code the database understands.
* Parsing takes time and impacts how long it takes for a query to return data. Effective use of parameterization reduces the number of times the database has to parse individual queries.
* Indexing -  A database index works like the index in the back of a book.
* Instead of looking at each page in a book to find what you are looking for, you can find a specific page number in the index and then go to that page.
A database index can point to a single column or multiple columns. When running queries on large tables, it is ideal if all of the columns you are retrieving exist in the index.
If that is not feasible, you at least want the first column in your SELECT statement to be covered by an index.
* While indexing improves query speed, it slows down create, update, and delete activity.
*  An indexing strategy needs to match the type of system the database supports, be it transactional or reporting.
*  Data Subsets and Temporary Tables - It is possible to create a temporary table to make the data more manageable.
*   Temporary tables can store the results of a query and are disposable. Temporary tables automatically get removed when the active session ends. Using temporary tables is an effective method of creating subsets for ad hoc analysis.
*   Execution Plan - An execution plan shows the details of how a database runs a specific query.
*   Execution plans are extremely helpful in troubleshooting query performance issues. They provide additional information about how a query is spending its time. For example, an execution plan can tell you if a slow-running query uses a full table scan instead of an index scan. 


# CHAPTER FOUR - DATA QUALITY
* Businesses need high-quality data to create the kinds of analysis that organizations rely on for decision-making.
* While the data we use in these analyses is not ever perfect, understanding the limitations of each dataset you use will help you identify any data transformation work that you must complete before proceeding with analysis.

# Data Quality Challenges
* Duplicate Data -occurs when data representing the same transaction is accidentally duplicated within a system. 
* Humans are primarily responsible for creating duplicate data.
*  System architects work diligently to prevent duplicate data from being created.
*  The best way to resolve duplicate data is to prevent its creation in the first place.
*  One common approach to stopping duplicate data before it gets into a system is a visual warning to alert users.
*   Having multiple data sources for the same data elements is also a source of duplicate data. 


* Redundant Data
* While duplicate data typically comes from accidental data entry, redundant data happens when the same data elements exist in multiple places within a system.
* data redundancy is a function of integrating multiple systems.
* One approach to reolve redundant data is to synchronizes changes to shared data elements between the Accounting and Sales systems.
*  However, technical or political realities can make synchronizing source systems unfeasible.
*  Another root cause of data redundancy is an inappropriate database design. 

* Missing Values
*  Missing values occur when you expect an attribute to contain data but nothing is there.
*  Missing values are also known as null values.
*   A null value is the absence of a value. A null is not a space, blank, or other character.
*   When a column optionally contains data, it is nullable, meaning the column can contain null values.
*    However, be aware that having nulls in a dataset poses calculation challenges.
*    Null values present several challenges depending on the tools you use to analyze data.
*    To handle missing values, you first have to check for their existence. SQL offers functions to check for null and functions that can replace a null with a user-specified value.
*    There are similar functions in both Python and R.

*  Invalid Data
*  Invalid data are values outside the valid range for a given attribute.
*   An invalid value violates a business rule instead of having an incorrect data type.
*   As such, you have to understand the context of a system to determine whether or not a value is invalid.
*   Invalid values violate business rules, not technical rules.
*    As such, programming languages do not have native functions that definitively tell you whether or not a given value is invalid.
*    When considering data types, numeric and date data is comparatively easy to check for invalid values.
*    One thing that leads to invalid character data is an absence of referential integrity within a database.

*    Nonparametric Data
*   Nonparametric data is data collected from categorical variables

*  Data Outliers
*  A data outlier is a value that differs significantly from other observations in a dataset.
* With outliers, you need to understand why they exist and whether they are valid in the context of your analysis.
* Outliers exist regardless of data type.

* Specification Mismatch
* A specification describes the target value for a component.
*  A specification mismatch occurs when an individual component's characteristics are beyond the range of acceptable values.
*  a specification mismatch occurs when data does not conform to its destination data type. For example, you might be loading data from a file into a database.
*   If the destination column is numeric and you have text data, you'll end up with a specification mismatch.
*   To resolve this mismatch, you must validate that the inbound data consistently maps to its target data type.

*   Data Type Validation
*  Data type validation ensures that values in a dataset have a consistent data type.
*  How the load process handles the data type validation failure determines whether or not the remaining rows load successfully.
*  Depending on the tool, a single failure may cause the load process to stop.
*  Alternatively, the load process might write each failed record to an error file before loading the remaining records.
*  Programming languages, including SQL, Python, and R, all have data type validation functions.
*   Use these functions to validate the data type for each column in a data file before attempting a database load.
*    detect and remediate data type issues as early as possible to ensure data is ready for analysis.


# Data Manipulation Techniques
* Recoding Data
*  is a technique you can use to map original values for a variable into new values to facilitate analysis.
*   Recoding groups data into multiple categories, creating a categorical variable. A categorical variable is either nominal or ordinal.
*   Nominal variables are any variable with two or more categories where there is no natural order of the categories, like hair color or eye color.
*   Ordinal variables are categories with an inherent rank. For example, T-shirt size is an example of an ordinal variable, as sizes come in small, medium, large, and extra-large.
*   Recoding is helpful when you have numeric data you want to analyze by category.

*   Derived Variables
*   A derived variable is a new variable resulting from a calculation on an existing variable.

*  Data Merge
*   data merge uses a common variable to combine multiple datasets with different structures into a single dataset.
*    Merging data improves data quality by adding new variables to your existing data.
*    Additional variables make for a richer dataset, which positively impacts the quality of your analysis.
*    ETL processes commonly append data while transforming data for use in analytical environments.
*    Since a data merge adds columns to a dataset, merging gives you additional data about a specific observation.

*    Data Blending
*    Data blending combines multiple sources of data into a single dataset at the reporting layer.
*    While data blending is conceptually similar to the extract, transform, and load process
*    ETL processes operate on a schedule, copying data from source systems into analytics environments.
*    Business requirements drive the scheduling, such as near real-time, hourly, daily, weekly, monthly, or annually. Typically, an organization's IT department designs, builds, operates, and maintains ETL processes.
*    Data blending differs from ETL in that it allows an analyst to combine datasets in an ad hoc manner without saving the blended dataset in a relational database.
*  Instead of the blended dataset persisting over time, it exists only at the reporting layer, not in the source databases.
*  data visualization tools such as Tableau allow analysts to connect to different source systems and blend the data using a shared attribute.
*   Data blending can reduce the burden on IT as it gives analysts the ability to merge data.
*   With the traditional ETL workflow, the analyst needs to understand the data warehouse's structure to create a visualization.
*   For routine analysis, such as weekly profitability, the ETL approach works well.
*   ETL/ELT processes are programmatic and operate on a schedule, resulting in a merged dataset that persists at the data layer.
*    After creation, ETL/ELT processes perform the same action on a routine basis. While conceptually similar, data blending combines data at the visualization layer and allows an analyst to integrate additional data sources in an ad hoc, exploratory manner.
*  ETL/ELT connects data at the database layer whereas data blending connects data at the visualization layer.

*  Concatenation
*  Concatenation is the merging of separate variables into a single variable.
*   Concatenation is a highly effective technique when dealing with a source system that stores components of a single variable in multiple columns.
*    The need for concatenation frequently occurs when dealing with date and time data. Concatenation is also useful when generating address information.
*    Combining this data is a straightforward activity, as programming languages, including SQL, Python, and R, have functions that make concatenation easy.

*    Data Append
*    data append combines multiple data sources with the same structure, resulting in a new dataset containing all the rows from the original datasets.
*    When appending data, you save the result as a new dataset for ongoing analysis.

*    Imputation
*    Imputation is a technique for dealing with missing values by replacing them with substitutes.
*    When merging multiple data sources, you may end up with a dataset with many nulls in a given column.
*    If you are collecting sensor data, it is possible to have missing values due to collection or transmission issues.
* approaches an analyst can use for imputing values:
*      Remove Missing Data:  With this approach, you can remove rows with missing values without impacting the quality of your overall analysis.
*     Replace with Zero:  With this approach, you replace missing values with a zero. Whether or not it is appropriate to replace missing data with a zero is contextual.
*     Replace with Overall Average:  Instead of using a zero, you can compute the average Weight value for all rows that have data and then replace the missing Weight values with that calculated average.
*     Replace with Most Frequent (Mode):  Alternatively, you can take the most frequently occurring value, called the mode, and use that as the constant
*     Closest Value Average:  With this approach, you use the values from the rows before and after the missing values. 

* Reduction
* When dealing with big data, it is frequently unfeasible and inefficient to manipulate the entire dataset during analysis.
*  Reduction is the process of shrinking an extensive dataset without negatively impacting its analytical value.
*  There are a variety of reduction techniques from which you can choose. Selecting a method depends on the type of data you have and what you are trying to analyze.
*   Dimensionality reduction and numerosity reduction are two techniques for data reduction.

*   Dimensionality Reduction
*   One reduction technique is dimensionality reduction, which removes attributes from a dataset. Removing attributes reduces the dataset's overall size.
*    can use any programming language, including Python or R, to remove dimensions.

*    Numerosity Reduction
*    Another technique is numerosity reduction, which reduces the overall volume of data.
*    As data volumes grow, numerosity reduction can improve the efficiency of your analysis.
*    One way to reduce the volume of quantitative data is by creating a histogram.
*    can create a histogram in Python, R, and many visualization-specific tools.
*    A histogram is a diagram made up of rectangles, or bars, that show how frequently a specific value occurs.

*    Aggregation
*    Data aggregation is the summarization of raw data for analysis.
*    Aggregating data provides answers that help make decisions
*    Aggregation is also a means of controlling privacy.

*    Transposition
*    Transposing data is when you want to turn rows into columns or columns into rows to facilitate analysis.
* Combining aggregation with transposition is a powerful data manipulation technique.
*  With this approach to organizing the data, you get a column for each fiscal year and total sales for each salesperson by territory.
*  This data representation makes it easy to view performance across fiscal years at a glance. This format also makes it easier to visualize data

* Normalization
* normalizing data converts data from different scales to the same scale.
*  If you want to compare columns whose measurements use different units, you want to normalize the data.
*  After normalization is complete, the dataset is ready for statistical analysis.

* Parsing/String Manipulation
* Raw data can contain columns with composite or distributed structural issues.
*  A composite issue is when a raw data source has multiple, distinct values combined within a single character column.
*  When this happens, each value in a composite column has data that represents more than one attribute. Composite columns need to be split into their component parts to aid nalysis.
*  You also may need to manipulate string data to improve data quality.

#   Managing Data Quality

* Circumstances to Check for Quality
* Errors during data acquisition, transformation, manipulation, and visualization all contribute to degrading data quality.
* Must recognize the types of quality issues that can occur and have an overarching strategy to ensure the quality of your data.
* Data Acquisition, Data Transformation and Conversion, Data Manipulation, Final Product Preparation

* Automated Validation
*  Whether source data is machine- or human-generated, one way to prevent data entry mistakes from adversely impacting data quality is to automate data validation checks.
*  Before automatically validating input data, you need to understand how source data fields map to their corresponding database columns.
*   When mapping input data, pay close attention to the data types in the database. For example, suppose you have a web form where customers supply phone numbers, and the destination database uses a numeric data type to store phone

*  Data Quality Dimensions
*   It is essential to consider multiple attributes of data when considering its quality. Six dimensions to take into account when assessing data quality are accuracy, completeness, consistency, timeliness, uniqueness, and validity.

*  Data Quality Rules and Metrics
*  With an understanding of data quality dimensions, you need to consider how to measure each of them in your quest to improve overall quality.
*    data conformity, which encompasses elements of accuracy, consistency, uniqueness, and validity.
*  When consolidating data from multiple source systems into an analytics environment, one factor you want to assess is the conformity or nonconformity of data.
*  If source data does not match the destination data type size and format, you have nonconformity.

*  Methods to Validate Quality
*  Reasonable Expectations - One approach is to determine whether or not the data in your analytics environment meets your reasonable expectations. 
*  After defining how you want to measure your expectations, automate the reasonable expectation check by creating exception reports as part of your ETL processes.
*  The root cause of the ETL load failure needs remediation to prevent ongoing issues with data quality.

* Data Profiling
*  approach to improving quality is to profile your data. Data profiling uses statistical measures to check for data discrepancies, including values that are missing, that occur either infrequently or too frequently, or that should be eliminated.
*  Profiling can also identify irregular patterns within your data.
*  The results of your data profiling activity fail the reasonable expectation test, as customers typically log in less frequently and from fewer devices.
*   Instead of trusting this data, you proceed to investigate whether or not this activity is fraudulent.

*  Data Audits
*  Data audits look at your data and help you understand whether or not you have the data you need to operate your business.
*   Data audits use data profiling techniques and can help identify data integrity and security issues.
*    For example, suppose you work with a large company that has relationships with numerous suppliers. To understand what is reasonable, you create a report to show the average payment amount by the supplier.

* Sampling
* Sampling is a statistical technique in which you use a subset of your data to inform conclusions about your overall data.

*  Cross-Validation
*  Analysts frequently use existing data to generate predictive models using a variety of statistical methods.
*  Cross-validation is a statistical technique that evaluates how well predictive models perform. Cross-validation works by dividing data into two subsets.
*  The first subset is the training set, and the second is the testing, or validation, set.
*   use data from the training set to build a predictive model.
*  cross-validate the model using the testing subset to determine how accurate the prediction is. Cross-validation is also helpful in identifying data sampling issues.
*  

# CHAPTER FIVE : Data Analysis and Statistics

* Fundamentals of Statistics
* core statistical concepts

* population represents all the data subjects you want to analyze.
* sample is a subset of the population.
* variable is a unique attribute of a data subject.
* Univariate analysis is when you explore the characteristics of a single variable, independent of the rest of the dataset.
* An observation is an individual record in a dataset corresponding to a tabular data row.
*  sample size is the number of observations you select from the population.
*  The larger the sample size, the more confident you can be that the results of your analysis accurately describe the population.
* statistic is a numeric representation of a property of a sample.
*  use statistics to infer estimates about the population as a whole.
*  also use a sample statistic to estimate a population parameter.
*   parameter is a numeric representation of a property for the population.
*   Just as statistics summarize sample information, parameters summarize the entire population.

*   Common Symbols in Statistics
*   Statistics is all about exploring numbers and performing calculations
*   Symbol in statistics
* x-A variable
* StartAbsoluteValue x EndAbsoluteValue - Absolute value of a variable
* The absolute value of a number is always positive, so StartAbsoluteValue negative 5 EndAbsoluteValue equals 5.
* upper Sigma - Summation For example, sigma-summation x Subscript i denotes adding all observations of a variable together.
* N- Population size
* mu- Population mean
* sigma squared -Population variance
* sigma -Population standard deviation
* n -Sample size
* x overbar- Sample mean
* x overTilde- Sample median
* s squared -Sample variance
* s- Sample standard deviation
* upper C- Confidence level value
* upper -Standardized score
* alpha - Significance level
* upper Z Subscript StartFraction
* 
# Descriptive Statistics
* Descriptive statistics  summarizes and describes data. 
* Use descriptive statistics as measures to help  understand the characteristics of dataset.

* Measures of Frequency
* Measures of frequency help you understand how often something happens.

* Count - Count function in everything from spreadsheets to programming languages.
* Understanding the total number of observations is a frequently performed task.
* Google Sheet - counta(cell range) - Counts the number of values in a dataset, excluding null values
* Microsoft Excel - counta(cell range) - Counts the number of values in a dataset, excluding null values
* SQL - count(*) -Counts the number of rows in a table
* R - nrow(data frame) -Counts the number of rows in a data frame
* Python - len(data frame) - Counts the number of rows in a data frame

* Percentage
* percentage is a frequency measure that identifies the proportion of a given value for a variable with respect to the total number of rows in the dataset.
* To calculate a percentage, need the total number of observations and the total number of observations for a specific value of a variable.
* Exploring percentages also gives you a better understanding of the composition of your data and can help identify any biases in your dataset.
*  When data has a bias, your sample data isn't representative of the overall population you are studying.
*  Understanding proportions across a dataset aids in determining how you proceed with your analysis.

* Frequency
* Frequency describes how often a specific value for a variable occurs in a dataset.
* typically explore frequency when conducting univariate analysis.
*  The histogram is an optimal way to visualize frequency for continuous data.

* Measures of Central Tendency
*  use measures of central tendency to identify the central, or most typical, value in a dataset.
*   There are numerous ways to measure central tendency, and you end up using them in conjunction with each other to understand the shape of your data.
*   Mean
* mean, or average, is a measurement of central tendency that computes the arithmetic average for a given set of numeric values.
*  To calculate the mean, you take the sum of all values for an observation and divide by the number of observations.
*  Data analysis tools, including spreadsheets, programming languages, and visualization tools, all have functions that calculate the mean.
*  While the mean is one of the most common measurements of central tendency, remember that you can only calculate a mean for quantitative data.
*  be mindful of the effect outliers have on the mean's value. An outlier is a value that differs significantly from the other values of the same observation.
*  It is a best practice to check your data and account for outliers when using the mean.

* Median
* median identifies the midpoint value for all observations of a variable.
*  The first step to calculating the median is sorting your data numerically.
*  Once you have an ordered list of values, the next step depends on whether you have an even or an odd number of observations for a variable.
*  Identifying the median for an odd number of observations is straightforward—you just select the number in the middle of the ordered list of values.
*  Mathematically, you add one to the total number of values, divide by 2, and retrieve the value for that observation.
* For datasets with an even number of observations, you need to take the average of the two observations closest to the midpoint of the ordered list.

* Mode
* mode is a variable's most frequently occurring observation.
* Depending on  data, you may not have a mode
* Depending on the level of precision and amount of data, the mode may not facilitate insight when working with numeric data.
* the mode is more applicable when working with categorical data.

* Measures of Dispersion
* five common measures of dispersion:
* RANGE
*  range of a variable is the difference between its maximum and minimum values.
*  Understanding can help you determine what to do with outlier values.
*   It can also identify invalid values in your data. Spreadsheets and programming languages have functions available to identify minimum and maximum values.

*   DISTRIBUTION
*    probability distribution, or distribution, is a function that illustrates probable values for a variable, and the frequency with which they occur.
*  Histograms are an effective tool to visualize a distribution, because the shape provides additional insight into your data and how to proceed with analysis.
*  Distributions have many possible shapes, including normal, skewed, and bimodal.
*  Normal Distribution -  normal distribution is symmetrically dispersed around its mean, which gives it a distinctive bell-like shape. Due to its shape, the normal distribution is also known as a “bell curve.”
*  is applicable across a number of disciplines due to the central limit theorem (CLT), a foundational theorem for statistical analysis.
*  According to the CLT, as sample size increases, it becomes increasingly likely that the sampling distribution of all those means will be normally distributed.
*  The sampling distribution of the mean will be normal regardless of sample size if the parent population is normal. However, if you have a skewed parent population, then having a “sufficiently large” sample size may be needed to get a normally distributed sampling distribution.
*   Most people define sufficiently large as 30 or more observations in your sample. Because of the CLT, the normal distribution applies across a wide variety of attributes.
* Skewed Distribution
* skewed distribution has an asymmetrical shape, with a single peak and a long tail on one side.
* Skewed distributions have either a right (positive) or left (negative) skew. When the skew is to the right, the mean is typically greater than the median. On the other hand, a distribution with a left skew typically has a mean less than the median.
* Bimodal Distribution
* bimodal distribution has two distinct modes, whereas a multimodal distribution has multiple distinct modes.
* When you visualize a bimodal distribution, you see two separate peaks.
* Variance
* Variance is a measure of dispersion that takes the values for each observation in a dataset and calculates how far away they are from the mean value.
* This dispersion measure indicates how spread out the data is in squared units. Mathematically,  signifies population variance, which you calculate by taking the average squared deviation of each value from the mean
* Standard Deviation
* Standard deviation is a statistic that measures dispersion in terms of how far values of a variable are from its mean.
* standard deviation is the average deviation between individual values and the mean. Mathematically,  signifies population standard deviation, which you calculate by taking the square root of the variance
* calculating variance is an important step on the way to determining standard deviation. Similar to variance, the standard deviation is a measure of volatility, with a low value implying stability.
*  Standard deviation is a popular statistic because of the empirical rule.
*  Also known as the three-sigma rule, the empirical rule states that almost every observation falls within three standard deviations of the mean in a normal distribution.

*  Each Sample is Unique
*  each sample from a population is unique.
*  Special Normal Distributions - The Central Limit Theorem and empirical rule combine to make the normal distribution the most important distribution in statistics.
*  Standard Normal Distribution
*  The standard normal distribution, or Z-distribution, is a special normal distribution with a mean of 0 and a standard deviation of 1.
* can standardize any normal distribution by converting its values into Z-scores.
* Converting to the standard normal lets you compare normal distributions with different means and standard deviations.
* Student's T-Distribution
* The Student's t-distribution, commonly known as the t-distribution, is similar to the standard normal distribution in that it has a mean of 0 with a bell-like shape.
* One way the t-distribution differs from the standard normal distribution is how thick the tails are since you can use the t-distribution for sample sizes of less than 30.

* Measures of Position
* Understanding a specific value for a variable relative to the other values for that variable gives you an indication of the organization of your data.
*  Statisticians commonly use quartiles to describe a specific observation's position.
*  The process of obtaining quartiles is similar to that of determining the median. You first sort a numeric dataset from smallest to largest and divide it positionally into four equal groups. Each grouping is known as a quartile.
*   The first quartile is the group that starts with the minimum value, whereas the fourth quartile is the group that ends with the maximum value.
*   The interquartile range (IQR) is the combination of the second and third quartiles and contains 50 percent of the values in the data.
*   When exploring a dataset, recall that outliers can have a significant impact on mean and range.
*    Using the IQR as a dispersion indicator, in addition to the range, improves your perspective since the IQR excludes outliers.

  # INFERENTIAL STATISTICS
*    Inferential statistics is a branch of statistics that uses sample data to draw conclusions about the overall population.

*  Confidence Intervals
* confidence interval describes the possibility that a sample statistic contains the true population parameter in a range of values around the mean.
*  When calculating a confidence interval, you end up with a lower bound value and an upper bound value.
*   Given the confidence interval range, the lower bound is the lower limit, and the upper bound is the upper limit.

*   Hypothesis Testing
*   approaches to proving or disproving ideas is hypothesis testing. A hypothesis test consists of two statements, only one of which can be true.
*   It uses statistical analysis of observable data to determine which of the two statements is most likely to be true.
*   when hypothesis testing, the null and alternative hypothesis describe the effect in terms of the total population. To perform the hypothesis test itself, you need sample data to make inferences about characteristics of the overall population.
*  A hypothesis test consists of two components: the null hypothesis and the alternative hypothesis. When designing a hypothesis test, you first develop the null hypothesis.
*   A null hypothesis (H0) presumes that there is no effect on the test you are conducting.
*   When hypothesis testing, your default assumption is that the null hypothesis is valid and that you have to have evidence to reject it.
*    The alternative hypothesis (Ha) presumes that the test you are conducting has an effect.
*   To determine the statistical significance of whether to accept or reject the null hypothesis, you need to compare a test statistic against a critical value.
*    A test statistic is a single numeric value that describes how closely your sample data matches the distribution of data under the null hypothesis.

*  Simple Linear Regression
*  Simple linear regression is an analysis technique that explores the relationship between an independent variable and a dependent variable.
*  You can use linear regression to identify whether the independent variable is a good predictor of the dependent variable.
*  can perform a regression analysis in spreadsheets like Microsoft Excel and programming languages, including Python and R. When plotting the results of a regression, the independent variable is on the x-axis and the dependent variable is on the y-axis.

*  From Simple to Multiple Linear Regression
*  Multiple linear regression builds on that concept by examining the effect of numerous independent variables on a dependent variable
*   crucial aspect of linear regression is the correlation between how far the observations are from the regression line.
*   Correlation is a measurement of how well the regression line fits the observations. The correlation coefficient (r) ranges between –1 and 1 and indicates the strength of the correlation.
*   The stronger the correlation, the more tightly the points wind around the line of best fit.
*   Perfect correlation is when r has a value of either –1 or 1, implying that every data point falls directly on the regression line. Interpreting correlation strength depends on the industry.
*  When evaluating the correlation between variables, one thing to keep in mind is that it does not imply causation.

  # NALYSIS TECHNIQUES
* Determine Type of Analysis
* When embarking on a new analytics challenge, you need to understand the business objectives and desired outcomes.
*  This understanding informs the type of analysis you will conduct.
*   The first step to understanding the objectives is to ensure that you have clarity on the business questions at hand.
*   need to review the business questions and identify any points that require additional clarification. This clarity will help you identify the data you need as well as the data sources.
*  While reviewing requirements, develop a list of clarifying questions. This list can help define the scope of your analysis.
*   Your clarification list can also identify any gaps between what is achievable given data source and time constraints.
*   Once you have your list of questions, review it with the business sponsor to ensure you agree on expectations.
*   Recognize that reviewing and refining business questions is an iterative process.
*   While you need to have initial clarity, you will likely have to return to your business leader for additional clarification as you conduct your analysis.
*   Use the document to track new issues that impact the project timeline and any adjustments to project scope or ultimate deliverables.
*   A running log identifying any scope changes is a valuable aid that can help you make sure you deliver your analysis on time.
*   It can also help you after your work is complete as you reflect on what went well and what would make future endeavors more successful.

*  Types of Analysis
*  One of the types of analysis you may be asked to perform is trend analysis.
*  Trend analysis seeks to identify patterns by comparing data over time.
*  In addition to trend analysis, you may also conduct performance analysis.
*  Performance analysis examines defined goals and measures performance against them and can inform the development of future projections.
*  Performance analysis can identify whether properties are achieving those goals.
*  Combining performance analysis with trend analysis can help develop projections for the future.
# Exploratory Data Analysis
*   An exploratory data analysis (EDA) uses descriptive statistics to summarize the main characteristics of a dataset, identify outliers, and give you context for further analysis.

*  approaches to conducting an EDA:
*  Check Data Structure:  Ensure that data is in the correct format for analysis. Most analysis tools expect data to be in a tabular format, so you need to confirm that your data has defined rows and columns.\
*  Check Data Representation:  Become familiar with the data. In this step, you validate data types and ensure that variables contain the data you expect.
*  Check if Data Is Missing:  Check to see if any data is missing from the dataset and determine what to do next. While checking for null values, calculate the proportion of each variable that is missing. If you discover that most of the data you need is missing, you need to either categorize it as missing or impute a value for the missing data. You can also go back to the source and remediate any data extraction issues.
* Identify Outliers:   outliers can dramatically impact some descriptive statistics, like the mean. It would be best to determine the cause of outliers and consider whether you want to leave them in the data before proceeding with any ongoing analysis.
* Summarize Statistics:  Calculate summary statistics for each variable. For numeric variables, examples of summary statistics include mean, median, and variance. For categorical data like eye colour, you could develop a table showing the frequency with which each observation occurs.
* Check Assumptions:  Depending on the statistical method you are using, you need to understand the shape of the data. For example, if you are working with numeric data, you should choose a normal or t-distribution for drawing inferences. If you are working with categorical data, use the chi-square distribution.

# DATA ANALYTICS TOOLS- CHAPTER 6
* When you are selecting a tool you shouls consider.
* First, you need the right tool for the job. an analytics professional would not use a spreadsheet to create a machine-learning model.
* Second, you need to choose from the tools available to you. Many analytics tools come with hefty price tags and organizations only license a small subset of them to control costs. Standardizing on a subset of tools also helps improve the ability of teams to work together.

* Spreadsheets
* spreadsheet software is installed on pretty much every computer in the modern work environment, and web-based spreadsheets are freely available to anyone.
* Spreadsheets are productivity software packages that allow users to create documents that organize any type of data into rows and columns.
* Users may place any data they like in the spreadsheet and then quickly and easily perform mathematical calculations, such as finding the sum of the values in a row or searching out the minimum, maximum, mean, and median values in a dataset.
* Spreadsheets lack any of the constraints of a relational database.
* While you can certainly organize data in a spreadsheet, there's no requirement that you do so.
*  If you'd like, you can mix numbers, text, dates, and other data elements all in the same column.
*   Microsoft Excel is the most commonly used desktop spreadsheet application.
*    It is available as a component of the widely deployed Microsoft Office productivity suite and most modern knowledge workers have access to it.
*    spreadsheet is organized in the same way you might organize a database table - each column represents a particular data element recorded about each inspection and each row represents a single inspection event.
* Excel  allows users to perform calculations and visualizations on their data.
*  Cloud-based spreadsheets such as Google Sheets are also quite popular because they make it easy for multiple people to collaborate on the same spreadsheet.

# Programming Languages
*  Programming languages allow skilled software developers to write their own instructions to the computer, allowing them to directly specify the actions that should take place during the analytics process.

*  R
*   R programming language is extremely popular among data analysts because it is focused on creating analytics applications
*    it is available to everyone as a free, open-source language developed by a community of committed developers.
*    This approach broke the mold of past approaches to analytic tools that relied on proprietary, commercial software that was often out of the financial reach of many individuals and organizations.
* One of the most important advances in the R language was the creation of a set of R packages known as the tidyverse by Hadley Wickham and other developers
* The tidyverse approach to data analysis simplifies the use of the language and makes it accessible to anyone willing to invest a few hours in learning some basic syntax.
* Modern R developers choose to write, test, and deploy their code using an integrated development environment (IDE) called RStudio.

* Python
* major difference between Python and R is that Python is a general-purpose programming language. This means that it is capable of creating software to meet just about any need you might imagine. can do everything from code a video game to perform a complex data analysis in Python.
* Python also has specialized libraries that focus on the needs of analysts and data scientists
* the Python Data Analysis Library (pandas) provides a set of tools for structuring and analyzing data.

* Structured Query Language (SQL)
* The Structured Query Language (SQL) is the language of databases.
*  Any time a developer, administrator, or end user interacts with a database, that interaction happens through the use of a SQL command.
*  major sublanguages of SQL :
*  Data Definition Language (DDL) is used mainly by developers and administrators. It's used to define the structure of the database itself. It doesn't work with the data inside a database, but it sets the ground rules for the database to function.
*   Data Manipulation Language (DML) is the subset of SQL commands that are used to work with the data inside of a database. They do not change the database structure, but they add, remove, and change the data inside a database.

* our DML commands:
* SELECT command is used to retrieve information from a database. It is the most commonly used command in SQL as it is used to pose queries to the database and retrieve the data that you're interested in working with.
* INSERT command is used to add new records to a database table. If you are adding a new employee, customer order, or marketing activity, the INSERT command allows you to add one or more rows to your database.
* UPDATE command is used to modify rows in the database. If you need to change something that is already stored in your database, the UPDATE command will do that.
* DELETE command is used to delete rows from a database table. Don't confuse this command with the DROP command. The DROP command deletes an entire database table, whereas the DELETE command just deletes certain rows from the table.

* a developer, administrator, or power user who knows SQL might directly access the database server and send it a SQL command for execution.
* This often happens through a graphical user interface, such as the Azure Data Studio interface
* Utilities like Azure Data Studio can do more than just retrieve data.
* They also offer a graphical way for database administrators to reconfigure a database.
* You can click through a series of menus to choose the changes you'd like to make to the database and the utility writes SQL commands that carry out your requests and sends them to the database.
*  computer software can interact with databases programmatically. This  means that software can send SQL commands to the database as part of its activity.

 # Statistics Packages
* software packages go beyond the simple statistical analyses that are possible in spreadsheets and provide access to advanced statistical environments that are accessible through a graphical user interface and/or a built-in scripting language.
* These software packages may be used by anyone interested in data analysis, but the reality is that they are mainly the domain of professional statisticians.

* IBM SPSS
*  SPSS is one of the oldest statistical software packages,
*  offers advanced statistical analysis, a vast library of machine learning algorithms, text analysis, open-source extensibility, integration with big data and seamless deployment into applications.
*  Its ease of use, flexibility and scalability make SPSS accessible to users of all skill levels.
*  What’s more, it’s suitable for projects of all sizes and levels of complexity, and can help you find new opportunities, improve efficiency and minimize risk.
*   IBM SPSS Statistics supports a top-down, hypothesis testing approach to your data, while IBM SPSS Modeler exposes patterns and models hidden in data through a bottom-up, hypothesis generation approach.
*   SPSS benefits
*   Intuitive user interface - Prepare and analyze data with an easy-to-use interface without having to write code.
*   Flexible licensing option-  choose from purchase options including subscription and traditional licenses.
*   Boost data science productivity-Empower coders, noncoders and analysts with visual data science tools.

* Stata
* Stata is yet another statistical analysis package that dates back and continues to be updated today.
*  It offers essentially the same features as SPSS and SAS and provides users with both a graphical interface and a command-line interface depending on their personal preference.
*  Stata is less widely used than the more popular SAS and SPSS tools.
*  Fast.
*  Accurate.
*   Easy to use.
* Stata is a complete, integrated software package that provides all your data science needs data manipulation, visualization, statistics, and automated reporting.

* Minitab
* Minitab shares most of the same features as SPSS, SAS, and Stata but fits into the same category as Stata - an older tool that is not widely used today.
* Minitab Statistical Software can look at current and past data to find trends and predict patterns, uncover hidden relationships between variables, visualize data interactions and identify important factors to answer even the most challenging of questions.
* Minitab can empower all parts of an organization to predict better outcomes, design better products and improve processes to generate higher revenues and reduce costs.
* Key statistical tests include t tests, one and two proportions, normality test, chi-square and equivalence tests.
* Access modern data analysis and explore your data even further with our advanced analytics and open source integration. Skillfully predict, compare alternatives and forecast your business with ease using our revolutionary predictive analytics techniques.
* Use classical methods in Minitab Statistical Software, integrate with open-source languages R or Python, or boost your capabilities further with machine learning algorithms like Classification and Regression Trees

![image](https://github.com/ZiyandaPetela/DATA-ANALYTICS/assets/123364433/7bab7b35-0487-4880-96e6-60cba0892bc4)
# Machine Learning
*  a set of graphical tools designed to help analysts build machine learning models without requiring them to actually write the code to do so.

* IBM SPSS Modeler
* IBM's SPSS Modeler is one popular tool for building graphical machine learning models. Instead of requiring that users write code, it provides an intuitive interface where analysts can select the tasks that they would like the software to carry out and then connect them in a flowchart-style interface.
* Once the user is satisfied that they've designed the machine learning process, they then run the model and can visually explore the results

* RapidMiner
* RapidMiner is another graphical machine learning tool that works in a manner similar to IBM SPSS Modeler.
* It offers access to hundreds of different algorithms that may be placed in a visually designed machine-learning workflow.
* RapidMiner also offers prebuilt analytic templates for common business scenarios.

# Analytics Suites
* These tools allow analysts to ingest and clean data, perform exploratory statistical analysis, visualize their data, produce models, make predictions, and communicate and report their results.
* These packages are normally more expensive than the other tools we've discussed, but they also provide an end-to-end environment where all of an organization's analysts and developers may work together on analytics projects.

* IBM Cognos
* IBM Cognos is an example of one of these integrated analytics suites.
*  It uses a web-based platform to offer analysts within an organization access to their data and is backed by IBM's Watson artificial intelligence capability.
* major components of Cognos :
* Cognos Connection is a web-based portal that offers access to other elements of the Cognos suite.
* Query Studio provides access to data querying and basic reporting tools.
* Report Studio offers advanced report design tools for complex reporting needs.
* Analysis Studio enables advanced modeling and analytics on large datasets.
* Event Studio provides real-time data monitoring and alerting, allowing business leaders to be immediately notified when certain events take place and/or provide automated responses to those events.
* Metric Studio offers the ability to create scorecards for business leaders to quickly analyze key metrics from across the organization.
* Cognos Viewer allows stakeholders to easily interact with data and analyses prepared using Cognos.

* Microsoft Power BI
* Power BI is Microsoft's analytics suite built on the company's popular SQL Server database platform.
*  Power BI is popular among organizations that make widespread use of other Microsoft software because of its easy integration with those packages and cost-effective bundling within an organization's Microsoft enterprise license agreement.
*  major components of Power BI :
*  Power BI Desktop is a Windows application for data analysts, allowing them to interact with data and publish reports for others.
*  The Power BI service is Microsoft's software-as-a-service (SaaS) offering that hosts
*  Power BI capabilities in the cloud for customers to access.
*  Mobile apps for Power BI provide users of iOS, Android, and Windows devices with access to Power BI capabilities.
*  Power BI Report Builder allows developers to create paginated reports that are designed for printing, email, and other distribution methods.
*  Power BI Report Server offers organizations the ability to host their own Power BI environment on internal servers for stakeholders to access.

*  MicroStrategy
*  MicroStrategy is an analytics suite that is less well-known than similar tools from IBM and Microsoft, but it does have a well-established user base.
*  MicroStrategy offers many of the same tools as its counterparts, making it easy for users to build dashboards and reports and apply machine learning techniques to their business data.

*  Domo
* Domo is a software-as-a-service (SaaS) analytics platform that allows businesses to ingest their data and apply a variety of analytic and modeling capabilities. It is not a very widely used tool

*  Datorama
*  Datorama is an analytics tool that focuses on a specific component of an organization's business: sales and marketing.
*  It's not a general-purpose analytics tool but is instead focused on applying machine learning, visualization, and other analytics techniques to the sales and marketing process.

*  AWS QuickSight
*  AWS QuickSight is a dashboarding tool available as part of the Amazon Web Services cloud offering.
*  This tool's power comes from the fact that it is available on a pay-as-you-go basis and its integration with the powerful data storage, data warehousing, machine learning, and artificial intelligence capabilities offered by the Amazon cloud.
* Tableau
* Tableau is arguably the most popular data visualization tool available in the market today.
*  The focus of this tool is on the easy ingestion of data from a wide variety of sources and powerful visualization capabilities that allow analysts and business leaders to quickly identify trends in their data and drill down into specific details.

* Qlik
*  Qlik is another popular SaaS analytics platform, offering access to cloud-based analytics capabilities.
*  QlikView is the company's original analytics platform that focuses on providing rapid insights.
*  Qlik Sense is a more advanced platform providing more sophisticated analytics capabilities (at a higher cost, of course!).

*  BusinessObjects
* BusinessObjects is an enterprise reporting tool from SAP that is designed to provide a comprehensive reporting and analytics environment for organizations.
*  One of the strengths of this suite is the ability to integrate BusinessObjects reports with other applications, allowing organizations to integrate analytics into other portions of their workflow.

# SUMMARY QUESTIONS 
*  Describe the role of the spreadsheet in the modern organization.
*  Spreadsheets are productivity software packages that allow users to create documents that organize any type of data into rows and columns.
*  They are extremely flexible analytics tools that are available on most modern office computer systems and are very easy to use. The most commonly used spreadsheet software package is Microsoft Excel.
*  Understand how analytics teams use programming languages.
*  Data professionals with coding skills often turn to programming languages to create their software analysis tools.
*  This approach frees them of the constraints of other packages and allows them to create software that directly meets their needs.
*  The R programming language is designed specifically for analytics use and is quite easy to learn
*   Python is a general-purpose programming language that is more difficult to learn but can create virtually any software package
*   Know how analysts and developers interact with databases
*    Relational databases are the primary data stores used in the modern organization
*   Analysts and developers may interact directly with databases using the Structured Query Language (SQL). SQL has two subcomponents
*    The Data Definition Language (DDL) defines the structure of the database and contains commands to create, alter, and destroy databases and tables
*  The Data Manipulation Language (DML) interacts with the data stored in a database and contains commands to add, retrieve, modify, and delete data.
*  Describe the role of statistical analysis software.
*   Statistical analysis software provides access to advanced statistical environments that are accessible through a graphical user interface and/or a built-in scripting language.
*  These software packages are commonly used by professional statisticians in their work.
*  The statistical analysis software packages covered on the Data+ exam include IBM SPSS, SAS, Stata, and Minitab.
*  Describe the role of machine learning software. Machine learning packages offer a set of graphical tools designed to help analysts build machine learning models without requiring them to actually write the code to do so.
*   These machine-learning tools aim to make machine-learning techniques more accessible.
*   Analysts may still tune the parameters of their models but do not necessarily need to write scripts to do so. The machine learning software packages covered on the Data+ exam include IBM SPSS Modeler and RapidMiner.
*   Describe the role of data analytics suites. Data analytics suites provide powerful capabilities that cross all phases of an analytics process.
*    These tools allow analysts to ingest and clean data, perform exploratory statistical analysis, visualize their data, produce models, make predictions, and communicate and report their results.
*    The data analytics suites covered on the Data+ exam include IBM Cognos, Microsoft Power BI, MicroStrategy, Domo, Datorama, AWS QuikSight, Tableau, Qlik, and BusinessObjects


# CHAPTER SEVEN: DATA VISUALIZATION WITH REPORTS AND DASHBOARDS
* Understanding the Business Requirements
* Reports and dashboards both summarize data for end users, but they distribute those summaries in different ways
* A report is a static electronic or physical document that reflects information at a given point in time.
*  a dashboard is an interactive visualization that encourages people to explore data dynamically.
*  Both reports and dashboards are ideal tools for visualizing data content.
*  The most important thing to define when developing a report or dashboard is answering the question, “Who is the audience?”
*   You shouldn't begin creating a report or dashboard until you clearly unyearsderstand who your audience is and how they will use the product.
*   Once you clearly understand the audience and their needs, you can turn your attention to identifying the data sources that will satisfy the requirements of your audience. For instance, you may need to combine data across multiple subject areas.
*    Sourcing this data can be a challenge, as when a corporation grows inorganically through acquisition, it can end up with multiple redundant systems to accomplish a single business objective. As an analyst, your job is to ensure that you have access to the appropriate data sources.
*    Report parameters let you define data range limits for the data elements in your report.
*    Once you identify who needs what data and when they need it, you can focus on how people access the report.
*  If people access the report digitally, one way to solve the distribution challenge is with a pull approach.
*   With a pull approach, you publish a report to a known location, like a web page, and let people know the frequency and timing of when the report updates. With this approach, people can go to the website when they want to use the report.
*   Alternatively, you could implement a push approach. With a push approach, the report is automatically sent to the appropriate people as it becomes available.
*    When designing a push approach, you need to think through distribution considerations.
*    For example, a report may prove to be too large to distribute via email.
*  In that case, you could use a blended distribution approach. With a blended approach, you store the report centrally and let people know when the report has been updated and is ready for use.
*  With the blended approach, you inform people that the report is available while maintaining central control of the report itself.
*  If you go with a push or blended approach for informing people about the readiness of a given report, be sure to think through the maintenance of the distribution list of people to notify.
*   With all organizations, people rotate in and out of roles. As people transition out of a position, you want to ensure that they no longer receive notifications about reports that are no longer relevant to their job role. On the other hand, as a new person joins the organization, you need to get that person the reports they require to be effective in their role.

*   Understanding Report Design Elements
* When creating a report or a dashboard, you can use existing design principles as guideposts.
*  These design principles, known as the “five Cs” of creating visualizations, will help ensure that your reports and dashboards communicate clearly and efficiently.
*  When thinking visually, the five Cs are control, correctness, clarity, consistency, and concentration.
* Control has to do with how you focus the attention of your audience.
* When someone encounters a dashboard for the first time, one of your goals is to deliver the pertinent information quickly
*  For instance, if there is a place where people can adjust parameters and have the dashboard respond, use visual highlights to focus attention on this capability.
*  Correctness makes sure that your information is accurate and that there are no spelling mistakes. Pay close attention to correctness when using corporate names and logos.
*  Clarity refers to selecting the right visualization tool for communicating your message, making sure the visualization is easy to interpret and visually crisp, and using fonts and sizes that are easy to read.
*  Consistency refers to using the same design and documentation elements throughout your report or dashboard to give your visualization a cohesive and complete feel.
*   Using the same font, page layout, and web page design are all techniques for ensuring consistency.
*   Concentration refers to using visuals to focus your audience's attention on the most relevant information without overwhelming them with details.
*   Concentration, along with clarity and control, helps you focus your audience by reducing clutter and removing unnecessary details.
*    Use a layout that increases concentration and removes distracting visual elements from charts.

*  Report Cover Page
*  When developing a printed report, keep in mind that the first thing people see is the cover page.
*   Since the cover page is the first thing a person sees, it is vital that it sets expectations about the observations and insights the reader will find within.
*   Effective cover pages have a concise title that describes the contents of the report.
*   Ideally, a cover page will also communicate a significant insight from the report itself.
*   Accompanying the title page should be clear instructions on how to use the report.

*   Executive Summary
*   The executive summary provides an overview of the report's contents.
*    When crafting an executive summary, you should begin with the end in mind, summarizing crucial observations and insights.
*  With time as an executive's most precious resource, the summary needs to convey the big ideas, while the body of the report details the analysis that led to those ideas.

*  Design Elements
*  When developing a report or dashboard, you need to incorporate design elements into your thinking.
*   Color schemes, page layout, font size and style, charts, and corporate standards are among the many design elements you should consider.
*    These considerations apply if you are enhancing an existing report or creating a new dashboard.

*  Color Schemes
*   A color scheme is a limited selection of colors you use when creating a report or a dashboard.
*   The first decision to make is whether you need to use a monochromatic color palette or have the flexibility to use more than one color.
*    A monochromatic palette limits you to working with shades of a single color,
*    If you have the luxury of working with more than one color, selecting a complementary color palette is a sound choice.
*    A complementary palette starts with two contrasting colors. Examples of complementary colors are red and green, orange and blue, and yellow and purple.\
*    Whether you are working in monochrome or multiple colors, ensure that the font color contrasts with the background color to ensure readability.

*    Layouts
*    The layout of a report or dashboard determines the arrangement of its component parts.
*    It is crucial to consider approachability when thinking about the design. When developing the layout for a report, begin with a summary before diving into the supporting details. For a long, multipage report, use a table of contents so that the reader can efficiently navigate to a topic of interest, as well as headings for sections and subsections.
*    Use brief paragraphs and bullet points to focus the reader's attention.
*  Ensure parallel construction when developing bullet points. Parallel construction is when all bullet points use the same form and have the same style and approximate length.

* Fonts
* When choosing a font style, pick one that is easy for people to read by avoiding ornate fonts.
* After excluding ornate options, you need to decide between a serif or sans serif font style.
* In typography, a serif is a finishing detail for each letter in a typeface. A serif font style includes serifs (the curls), whereas a sans serif font style does not

* Graphics
* Using graphics to present summary information is a practical choice, whether creating a report or developing a dashboard.
*  As the saying goes, a picture is worth a thousand words, and visually conveying information with charts helps focus your audience's attention.
*  Think through the key chart elements when designing charts, including the chart's title, labels, and legends.
*  When using a chart, be sure to use labels appropriately. In a chart with an x-axis and a y-axis, a label describes what each axis represents.
* When a chart shows multiple categories, use a legend to help the reader distinguish between categories.

*  Corporate Reporting Standards
*  When developing any type of visualization, be mindful of any existing corporate reporting standards. 
For instance, your organization may have a style guide for reporting.
 A style guide is a set of standards that drives consistency in communication. As a means of enforcing structure and consistency, style guides define the use of a variety of branding elements, including page layout, font selection, corporate color codes, logos, and trademarks\

* Documentation Elements
* People must trust the information in your visualizations.
*  To help establish trust, you can incorporate documentation elements, including version numbers, reference data sources, and reference dates.
*   Reference dates include the initial creation date, report run date, and data refresh date.

* Version Number
*  A version number is a numeric value that refers to a specific version of a report.
*  Version numbers help you keep track of changes to content and layout over time.
*   Reference data sources identify where data in the report originates.
*   For example, if you are using a data mart to create a visualization for colleagues in the finance division, specify the name of the data mart.

*  Reference Data Sources
* Reference dates help people understand what to expect in terms of data recency.
*  For example, if a report has a daily refresh cycle, the report run date helps people realize when the last data refresh date was.
*   If they see that the refresh date is from a week ago, they know the report is missing a week's worth of data.

*  Frequently Asked Questions
*  When developing a report or a dashboard, it is good to maintain a set of frequently asked questions (FAQs).
*   A FAQ provides answers to people's most common questions.
*   If the dashboard is available online, the FAQ can contain links to a glossary of unique terms, cross-references to other dashboards or reports, and contact information if there are additional questions.

* Appendix
*  When developing a report, use an appendix to include supporting details that are inappropriate to include in the main body.
*  For example, suppose you use statistical analysis to derive the central insight for a report.
*  Recall that one of the goals of creating a report is to convey insights.
*  Instead of detailing each calculation in the main report body, move them to an appendix.
*   That way, the general reader will not feel overwhelmed by the details, while the informed reader can explore the calculations in detail.

*  Understanding Dashboard Development Methods
* dashboard considerations that you should keep in mind:
*  Consumer Types
*  As with developing a report, it is crucial to identify who will be interacting with the dashboard you create.
*   dashboard needs to consolidate critical performance metrics with the ability to get additional detail on an as-needed basis to assist people with C-level responsibilities in making strategic decisions.
*   Ensure you spend sufficient time identifying the key performance indicators (KPIs) crucial to senior leaders.
*   A KPI is a metric that leadership agrees is crucial to achieving the organization's business objectives. As you identify what leaders want to see, you can locate where to get the relevant data.
*   A large organization typically has external stakeholders who serve on its board of directors.
*    While the board's needs closely align with the needs of C-level executives, you need to incorporate any of the board's unique requirements in your dashboard so that board members can fulfill their oversight and corporate management duties effectively.

* Data Source Considerations
* With clarity on what your dashboard needs to contain, you can proceed with identifying data sources. The most vital determination you make about data sources hinges on whether or not the dashboard needs to incorporate live data. 
* static data is data that refreshes at some regular interval. A typical design pattern is for operational databases to update a data warehouse every night.
* Continuous data, also known as live data, typically comes directly from an operational database that people use to perform their daily duties.
*  The operational database provides a live data feed to the dashboard.   

* Data Type Considerations
* One thing that differentiates dashboards and reports is the fact that dashboards use software as the delivery mechanism.
*  As such, you have to have a deeper understanding of your source data than when you are creating a report.
*  Whether you use packaged software like Tableau or Qlik or write your own visualization using a programming language like Python or R, you need to ensure you can handle the data type of each attribute.
*  When creating a dashboard, you use qualitative data to create dimensions.
*  A dimension is an attribute that you use to divide your data into categories or segments.
*  To make sure you are representing the source data categories entirely, map the field definitions from the source data to your visualization tool.
*   For example, a geographic dimension lets you look at your data by geographic region.
*   A date dimension enables you to explore data at various levels of time-related granularity. For example, suppose you are creating a sales dashboard and want to allow the flexibility to look at sales by day of the week, week, month, quarter, year, fiscal quarter, and fiscal year. Each date increment is a value within the time dimension.
*   A measure is a numeric, quantitative value that a dashboard user is curious about. 

* Development Process
* After you identify the data sources that will power your dashboard, you must turn your attention to developing the dashboard itself. Use wireframes and mock-ups to help build and refine the dashboard's design.
* A wireframe is a blueprint for an application that defines the basic design and functions of a dashboard.
* Think of a wireframe as a blueprint for a building. Architects develop blueprints to describe the internal structure of a building.
* Similarly, a wireframe defines the basic structure, functionality, and content of a dashboard. Use wireframes to define the presentation layout and navigation flow to guide how people will interact with the dashboard.
* A mock-up extends a wireframe by providing details about the visual elements of the dashboard, including fonts, colors, logos, graphics, and page styles.
* While a wireframe is conceptually similar to a blueprint, a mock-up is closer to an architectural rendering.
* The goal of a mock-up is to give people a perspective as to the dashboard's final user interface. Use mock-ups to ensure your dashboard is consistent with your organizational communication standards.

* Delivery Considerations
*  As you document their requirements and develop mock-ups, you need to determine whether people can subscribe to changes
*  If subscription capability is a requirement, you need to have a system where people can opt-in to receive a notification when the underlying data changes.
* In addition to offering a subscription service, another delivery consideration is scheduled delivery
*  The level of interactivity needs to accommodate requirements from the data story plan.
*  As the dashboard conveys aggregated data, you need to understand the dimensions where dashboard users will want to explore additional details.
*  That will help you design a dashboard that lets people drill down into deeper levels of detail.

* Operational Considerations
*  Once you have final approval, you proceed with developing the dashboard.
*   Similar to the design stage, make sure you include frequent opportunities to gather feedback.
*   Once dashboard development is complete, test it thoroughly to verify its functionality.
*   As you build a dashboard, make sure you clearly define the access permissions.
*    Access permissions define the data that a given person can access. When defining access permissions, do so in terms of roles instead of people
*  As you confirm that the dashboard will serve its intended purpose, you are ready to deploy it to production.
*  Once the dashboard is in active use, your work is not over.
*  You need to ensure the dashboard continues to answer leadership's questions while performing well.
*   As the person responsible for the dashboard itself, you need to pursue ongoing dashboard optimizations.
*   A given optimization may improve the performance of a dashboard component or may include a new data source to enable answering new sets of questions.  

# Exploring Visualization Types
* Charts
* Charts are one of the foundational methods for visualizing both qualitative and quantitative data.
  
* Line Chart - A line chart shows the relationship of two variables along an x- and a y-axis.
* Line charts effectively visualize the relationship between time on the x-axis and a variable on the y-axis.

* Pie Chart
* A pie chart gets its name from its circular shape where the circle represents 100 percent, and each slice of the pie is a proportion of the whole.
*  A pie chart presents categorical, or discrete, data as individual slices of the pie.
*  When using a pie chart, ensure that you label each pie slice appropriately

* Bar Chart
*  similar to a pie chart, a bar chart presents categorical data. Where a pie chart is circular, a bar chart uses rectangular bars to depict each proportion.
*   Bar charts tend to be more interpretable for people than pie charts

*   Stacked Chart -  stacked chart or stacked bar chart, starts with a bar chart and extends it by incorporating proportional segments on each bar for categorical data.
*  A scatter chart, or scatter plot, uses a dot for each observation in a data set to show the relationship between two numeric variables.
*   A bubble chart is a scatterplot where the size of each dot is dependent on a third numeric variable

*   Histogram
* A histogram is a chart that shows a frequency distribution for numeric data.
* When performing an exploratory data analysis, create histograms for numeric data.
* These histograms illustrate the shape of the distribution and can inform the next stage of analysis.
*  Histograms are also effective for communicating a distribution's shape to stakeholders.

*  Maps
* People frequently use maps to convey the location of a country, town, or individual address.
*  Maps are effective methods of orienting a person to a dataset.
*  There are numerous types of maps available to visualize data, including geographic, heat, and tree maps.

*  Waterfall
*  A waterfall chart displays the cumulative effect of numeric values over time.
*  Waterfall charts facilitate an understanding of how a series of events impact an initial value.
*  Use a waterfall chart any time you want to see how events affect a baseline value.

* Infographic
*  An infographic, which gets its name from the words “information” and “graphic,” is a visualization that presents information clearly and concisely.
 Infographics minimize text in favor of visual elements to represent a topic in a format that is easy to understand.
The goal of an infographic is to convey an insight in a way that minimizes the time to comprehension.

* Word Cloud
* A word cloud is a visualization that uses shape, font size, and color to signify the relative importance of words.
*  Word clouds are effective at visualizing free-form text responses.
*   When creating a word cloud, you eliminate common words and conjunctions as they occur frequently and don't add value in terms of meaning.
*    The heart shape of Figure 37 conveys positivity, and the words system, learn, data, inspired, course, and think stand out.

# Comparing Report  Types
* When embarking on any reporting project, recall that you first have to identify the audience and their needs.
* After clarifying who will consume your report and what information they need to see, it is crucial to determine when they need it.

* Static and Dynamic Report Types
* Static reports pull data from various data sources to reflect data at a specific point in time.
*  To feed your trend report, you need the daily price for each security over five years. Data marts and data warehouses are typical sources for this type of data.

*  Dynamic reports give people real-time access to information.
*   Using your five-year trend report to inform their analysis, a financial analyst in your company may want to execute a trade.
*    Data marts and data warehouses are insufficient for providing real-time information.
*  Ad hoc reports, or one-time reports, use existing data to meet a unique need at a specific point in time

*  Self-Service (On-Demand)



 



