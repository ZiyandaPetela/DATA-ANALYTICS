# DATA-ANALYTICS
# INTRODUCTION 
Confident computer user?
A confident computer user is one who knows more than just the required key presses to operate the software they are using.
* The two most important qualities of a confident computer user are:
   * Knowing how to cope when things go wrong
   * Knowing how to learn new computing skills independently
* Personal computing is changing so fast and becoming so complex that it never gets a chance to settle down and become really reliable.
* There are various ways of coping with computing problems:
   * Save your work often – every few minutes, not every few hours.  
   *  Back up all your important work regularly – if you work on your computer all day this should be done daily. This means making copies of files somewhere else.
   * Look for workarounds.  There is usually more than one way of getting what you want to be done

* Learning New Computing Skills
There are several principles for learning new computing skills:
Learning different computer skills and how to use different computer programs is actually fairly easy.  There are numerous websites and YouTube videos to show how. Some of the principles for learning new computing skills are to try things out and explore. 
Do not be afraid to try things out and explore. You almost certainly will not do any harm by making mistakes – modern software will usually let you undo errors and will warn you before performing potentially damaging actions. 
Look for patterns. Find the differences and similarities between the ways in which things are done in different software, so you can begin to guess what to expect.
You will learn a lot by helping other people in addition to getting your own problems solved by them. During this program, you will be able to develop your support network further by using the Internet. E-mail, chat, and the Web all greatly increase the ways in which you can seek help and information and, of course, help others.
# Data Analytics
Analytics is at the heart of modern business. Virtually every organization collects large quantities of data about its customers, products, employees, and service offerings. Managers naturally seek to analyze that data and harness the information it contains to improve the efficiency, effectiveness, and profitability of their work.
The ultimate role of a data analyst is to transform raw data into actionable insights that guide decision-making processes within an organization. This involves several key responsibilities and skills.

1. Data Collection and Preparation:

Sourcing data from various channels, including databases, spreadsheets, and external sources,
Cleaning and organizing the data to ensure it is accurate, consistent, and ready for analysis.
2. Data Analysis:

Employing statistical methods, machine learning techniques, or other analytic tools to interpret data,
Identifying trends, patterns, and correlations that might not be immediately obvious.
3. Data Visualization and Storytelling:

Creating visual representations of the data, such as charts, graphs, and dashboards, to make complex information easily understandable,
Articulating findings in a compelling narrative to communicate the significance of the data to stakeholders.
4. Decision Support:

Making recommendations based on data-driven insights to help guide business decisions,
Providing context around the data, including potential implications and future trends.
5. Collaboration and Communication:
Working closely with other departments, such as marketing, finance, and operations, to understand their data needs and provide insights,
Effectively communicating complex data findings in a clear and concise manner to non-technical stakeholders,
6. Continuous Learning and Adaptation:
Keeping up-to-date with the latest industry trends, tools, and technologies in data analysis.
Adapting to new types of data and analytical methods as the organization's needs evolve.


# World of Analytics
*  three major pillars that have come together at this moment to allow analytics programs to thrive: data, storage, and computing power.
*  Data -From the organized tables of spreadsheets to the storage of photos, video, and audio recordings, modern businesses create an almost overwhelming avalanche of data that is ripe for use in analytics programs.
*  Storage - The second key trend driving the growth of analytics programs is the increased availability of storage at rapidly decreasing costs.
*  Computing Power
#   Careers in Analytics
* Data analysts and scientists
* AI and machine learning (ML) specialists
* Big Data specialists
Digital marketing and strategy specialists
Process automation specialists
Business development professionals
Digital transformation specialists
Information security analyst
Software and applications developers
Internet of Things (IoT) specialists

# The Analytics Process
* Analysts working with data move through a series of different steps as they seek to gain business value from their organization's data
* Data Acquisition
* Cleaning and Manipulation
* Analysis
* Visualization
* Reporting and Communication
# The Analytics Process is Iterative
* The steps of the analytics process as a series of sequential actions, it is more accurate to think of them as a set of interrelated actions that may be revisited frequently while working with a dataset.
* This process is meant to help you understand the different activities that take place during a data analysis effort and the approximate order in which they typically occur.

# Analytics Techniques
* Descriptive Analytics
* Predictive Analytics
* Predictive Analytics
 # Machine Learning, Artificial Intelligence, and Deep Learning
 * Machine Learning, Artificial Intelligence, and Deep Learning
 * Artificial intelligence (AI) includes any type of technique where you are attempting to get a computer system to imitate human behavior.
 * Machine learning (ML) is a subset of AI techniques. ML techniques attempt to apply statistics to data problems in an effort to discover new knowledge.
 * Deep learning is a further subdivision of machine learning that uses quite complex techniques, known as neural networks, to discover knowledge in a particular way.

# Data Governance

* Data governance programs ensure that the organization has high-quality data and is able to effectively control that data.
# Analytics Tools
* Software helps analysts work through each one of the phases of the analytics process.
*  These tools automate much of the heavy lifting of data analysis, improving the analyst's ability to acquire, clean, manipulate, visualize, and analyze data.
*  They also provide invaluable assistance in reporting and communicating results.
# Understanding Data
# Exploring Data Types
* To understand data types, it is best first to understand data elements. A data element is an attribute about a person, place, or thing containing data within a range of values.
* Data elements also describe characteristics of activities, including orders, transactions, and events.
*  A data type limits the values a data element can have.
*  Tabular data is data organized into a table, made up of columns and rows. A table represents information about a single topic.
*  Each column represents a uniquely named field within a table, also called a variable, about a single characteristic.
*  Spreadsheets, including Microsoft Excel, Google Sheets, and Apple Numbers, are practical tools for representing tabular data.
*   A relational database management system (RDMS), commonly called a database, extends the tabular model. Instead of having all data in a single table, a database organizes related data across multiple tables.

# Structured Data Types
* Structured data is tabular in nature and organized into rows and columns.
* Structured data is what typically comes to mind when looking at a spreadsheet. With clearly defined column headings, spreadsheets are easy to work with and understand.
*  In a spreadsheet, cells are where columns and rows intersect.
# Character
* The character data type limits data entry to only valid characters.
* Characters can include the alphabet that you might see on your keyboard, as well as numbers. *Depending on your needs, multiple data types are available that can enforce character limits.
* alphanumeric is the most widely used data type for storing character-based data. As the name implies, alphanumeric is appropriate when a data element consists of both numbers and letters.

# Character Sets
* Numeric - When numbers exclusively make up values for a data attribute, numeric becomes the data type of choice. 
* Whole Numbers - The integer, and all its subtypes, are for storing whole numbers. 
* Rational Numbers - the numeric data type is for rational numbers that include a decimal point. 
* Date and Time - road category of date, day of year and time of day are data elements that appear with great frequency.
* Currency -financial data is numeric, people prefer seeing the numbers displayed as a specific currency. For example, consider the Number, Dollar, and Euro

# Strong And Weak Typing
* Strong typing is when technology rigidly enforces data types.
* Weak typing loosely enforces data types. Spreadsheets use weak typing to help make it easier for people to accomplish their work.
* Software that uses weak typing can be helpful.
  
# Unstructured Data Types
* Unstructured data is any type of data that does not fit neatly into the tabular model.
* Examples of unstructured data include digital images, audio recordings, video recordings, and open-ended survey responses.
*  Analyzing unstructured data creates a wealth of information and insight.
*  unstructured data type :
*  Binary
*  Audio
*  Images
*  Video
*  Large Text
# Categories of Data
* Quantitative vs. Qualitative Data - Quantitative data consists of numeric values. Data elements whose values come from counting or measuring are quantitative.Qualitative data consists of frequent text values. Data elements whose values describe characteristics, traits, and attitudes are all qualitative. 
* Discrete vs. Continuous Data - discrete data is that it represents measurements that can't be subdivided. You may intuitively think of discrete data as using whole numbers, but that doesn't have to be the case. when you measure things like height and weight, you are collecting continuous data. While whole numbers represent discrete data, continuous data typically need a decimal point. 
* Categorical Data - numeric data, there is categorical data. Text data with a known, finite number of categories is categorical.
* Dimensional Data - Dimensional modeling organizes data into fact tables and dimension tables. Fact tables store measurement data that is of interest to a business.

# Common Data Structures
* Structured Data - Tabular data is structured data, with values stored in a consistent, defined manner, organized into columns and rows.
* Unstructured data is qualitative, describing the characteristics of an event or an object. Images, phrases, audio or video recordings, and descriptive text are all examples of unstructured data. There is very little that is common about different kinds of unstructured data.
* Machine data is a common source of unstructured data. Machine data has various sources, including Internet of Things devices, smartphones, tablets, personal computers, and servers.
* Semi-structured data is data that has structure and that is not tabular. Email is a well-known example of semi-structured data.

# Common File Formats
* Common file formats facilitate data exchange and tool interoperability.
* Text files are one of the most commonly used data file formats. As the name implies, they consist of plain text and are limited in scope to alphanumeric data.
*  One of the reasons text files are so widely adopted is their ability to be opened regardless of platform or operating system without needing a proprietary piece of software.
*  Whether you are using a Microsoft Windows desktop, an Apple MacBook, or a Linux server, you can easily open a text file. Text files are also commonly referred to as flat files.
*  Fixed-Width Files - are more laborious to create since they require a few extra steps.
*  JavaScript Object Notation (JSON) is an open standard file format, designed to add structure to a text file without incurring significant overhead.
*   One of its design principles is that JSON is easily readable by people and easily parsed by modern programming languages.
*    Languages such as Python, R, and Go have libraries containing functions that facilitate reading and writing JSON files.
*   extensible Markup Language (XML) is a markup language that facilitates structuring data in a text file.
*  While conceptually similar to JSON, XML incurs more overhead because it makes extensive use of tags. Tags describe a data element and enclose each value for each data element.
* While these tags help readability, they add a significant amount of overhead.
* HyperText Markup Language (HTML) is a markup language for documents designed to be displayed in a web browser.
*  HTML pages serve as the foundation for how people interact with the World Wide Web. Similar to XML, HTML is a tag-based language.

# CHAPTER THREE - Databases and Data Acquisition
* two categories categories of a database 1.Relational 2.Nonrelational
* Relational databases excel at storing and processing structured data.
* The need to interact with unstructured data is one of the reasons behind the rise of nonrelational databases.
* Relational model builds on the concept of tabular data. In the relational model, an entity contains data for a single subject.
* structure has data points and bring out relationship there
* uses tables to show entities/nouns
* each table has ID to uniquely identify row
* benefits are consistency, stored procedures
* Each of these entities becomes a separate table in the database, with a column for each attribute.
*  Each row represents an instance of the entity.
*   The power of the relational model is that it also allows us to describe how entities connect or relate, to each other.
*   The entity relationship diagram (ERD) is a visual artifact of the data modeling process.
*    It shows the connection between related entities.
* a relationship is a connection between entities. The symbols adjacent to an entity describe the relationship.
# THE RELATIONAL MODEL
* The relational model builds on the concept of tabular data. In the relational model, an entity contains data for a single subject.
* The entity relationship diagram (ERD) is a visual artifact of the data modeling process. It shows the connection between related entities.
* A relationship is a connection between entities. The symbols adjacent to an entity describe the relationship.
* Cardinality refers to the relationship between two entities, showing how many instances of one entity relate to instances in another entity.
* specify cardinality in an ERD with various line endings.
* The first component of the terminator indicates whether the relationship between two entities is optional or required.
* The second component indicates whether an entity instance in the first table is associated with a single entity instance in the related table or if an association can exist with multiple entity instances.
* binary relationship connects two entities,
* ternary relationship connects three entities.

# Relational Databases
* Relational databases are pieces of software that let you make an operational system out of an ERD.
* Relational entities correspond to database tables, and entity attributes correspond to table columns.
* When creating a database table, the ordering of columns does not matter because you can specify the column order when retrieving data from a table.
*  When an attribute becomes a column, you assign it a data type. Completing all of this work results in a diagram known as a schema.
*  To pull data from a relational database table, you perform a query. You compose queries using a programming language called Structured Query Language (SQL).
*   A join uses data values from one table to retrieve associated data in another table, typically using a foreign key.
*  Foreign keys enforce referential integrity, or how consistent the data is in related tables
*   A database administrator (DBA) is a highly trained person who understands how database software interacts with computer hardware.
*   A DBA looks after how the database uses the underlying storage, memory, and processor resources assigned to the database.
*  A DBA also looks for processes that are slowing the entire database down.
*  Microsoft developed SQL Server, and the open source community created offerings including MySQL, MariaDB, and PostgreSQL.
*  Amazon Web Services (AWS) developed Aurora, which is compatible with MySQL and PostgreSQL.
*  Aurora is unique because it takes advantage of AWS's underlying cloud platform and is easy to scale.

# Nonrelational Databses
* A nonrelational database does not have a predefined structure based on tabular data. The result is a highly flexible approach to storing data. However, the data types available in relational databases are absent.
* Examples of nonrelational databases include key-value, document, column family, and graph.

# Database Use Cases
* two categories of data processing: Online Transactional Processing (OLTP) and Online Analytical Processing (OLAP).
* Online Transactional Processing
* OLTP systems handle the transactions we encounter every day.
* OLTP systems balance the ability to write and read data efficiently.
* Normalization
*  is a process for structuring a database in a way that minimizes duplication of data.
* One of the principles is that a given piece of data is stored once and only once. As a result, a normalized database is ideal for processing transactions.
*  First normal form (1NF) is when every row in a table is unique and every column contains a unique value.
* Second normal form (2NF) starts where 1NF leaves off. In addition to each row being unique, 2NF applies an additional rule stating that all nonprimary key values must depend on the entire primary key.
* Third normal form (3NF) builds upon 2NF by adding a rule stating all columns must depend on only the primary key. 

# Online Analytical Processing
* OLAP systems focus on the ability of organizations to analyze data
* While OLAP and OLTP databases can both use relational database technology, their structures are fundamentally different.
* OLTP databases need to balance transactional read and write performance, resulting in a highly normalized design. Typically, OLTP databases are in 3NF.
* Schema Concepts
* The design of a database schema depends on the purpose it serves.
* Transactional systems require highly normalized databases, whereas a denormalized design is more appropriate for analytical systems.
* A data warehouse is a database that aggregates data from many transactional systems for analytical purposes.
*  Transactional data may come from systems that power the human resources, sales, marketing, and product divisions.
*  A data warehouse facilitates analytics across the entire company.
*  A data mart is a subset of a data warehouse.
*   Data warehouses serve the entire organization, whereas data marts focus on the needs of a particular department within the organization.
*  A data lake stores raw data in its native format instead of conforming to a relational database structure.
*  Using a data lake is more complex than a data warehouse or data mart, as it requires additional knowledge about the raw data to make it analytically useful.
*  Relational databases enforce a structure that encapsulates business rules and business logic, both of which are missing in a data lake.
*  The star schema design to facilitate analytical processing gets its name from what the schema looks like when looking at its entity relationship diagram
*  At the centre of the star is a fact table. Fact tables chiefly store numerical facts about a business.
*  the schema design centres on reporting on the cost and profitability of procedures.
*  Qualitative data, including names, addresses, and descriptions, is stored in a series of dimension tables that connect to the main fact table.
*  When data moves from an OLTP design into a star schema, there is a significant amount of data duplication.
*   As such, a star schema consumes more space than its associated OLTP design to store the same data.
*  Another design pattern for data warehousing is the snowflake schema.
*   Snowflake and star schemas are conceptually similar in that they both have a central fact table surrounded by dimensions. \
*   Where the approaches differ is in the handling of dimensions. With a star, the dimension tables connect directly to the fact table.
*    With a snowflake, dimensions have subcategories, which gives the snowflake design its shape.
*    A snowflake schema is less denormalized than the star schema.
*    With a snowflake schema, you may need more than one join to get the data you are looking for.
*  A snowflake schema query is more complex than the equivalent query in a star schema.
*  Data warehouses often use snowflake schemas, since many different systems supply data to the warehouse.
*   Data marts are comparatively less complicated, because they represent a single data subject area. As such, data marts frequently use a star-schema approach. 

* Dimensionality
* Dimensionality refers to the number of attributes a table has. The greater the number of attributes, the higher the dimensionality.
* A dimension table provides additional context around data in fact tables
* Handling Dimensionality
* the start and end date approach - An understanding of this method is required to write a query to retrieve the current price.
* Another method extends the snowflake approach to modelling dimensions. You have a product dimension for the current price and a product history table for maintaining price history.
* One advantage of this approach is that it is easy to retrieve the current price while maintaining access to historical information.
* Another approach is to use an indicator flag - indicator flag method keeps all pricing data in a single place. It also simplifies the query structure to get the current price.

# Data Acquisition Concepts
* Integration
* Data from transactional systems flow into data warehouses and data marts for analysis.
* need to retrieve, reshape, and insert data to move data between operational and analytical environments.
* One approach is known as extract, transform, and load (ETL).
* Extract:  In the first phase, you extract data from the source system and place it in a staging area. The goal of the extract phase is to move data from a relational database into a flat file as quickly as possible.
* Transform:  The second phase transforms the data. The goal is to reformat the data from its transactional structure to the data warehouse's analytical design.
* Load:  The purpose of the load phase is to ensure data gets into the analytical system as quickly as possible.
* Extract, load, and transform (ELT) is a variant of ETL.
*  With ELT, data is extracted from a source database and loaded directly into the data warehouse.
*  Once the extract and load phases are complete, the transformation phase gets underway.
*  One key difference between ETL and ELT is the technical component performing the transformation.
*   With ETL, the data transformation takes place external to a relational database, using a programming language like Python. E
*   ELT uses SQL and the power of a relational database to reformat the data.
*   ELT has an advantage in the speed with which data moves from the operational to the analytical database.


*   ETL Vendors
*   An initial load occurs the first time data is put into a data warehouse.
*   After that initial load, each additional load is a delta load, also known as an incremental load.
*   A delta load only moves changes between systems.

# Data Collection Methods
*Application Programming Interfaces (APIs)- is a structured method for computer systems to exchange information. 
APIs provide a consistent interface to calling applications, regardless of the internal database structure. 
Whoever calls an API has no idea whether a transactional or analytical data store backs it. The internal data structure does not matter as long as the API returns the data you want. 
APIs can be transactional, returning data as JSON objects. 
APIs can also facilitate bulk data extraction, returning CSV files.

* web Services - data is found in private and public data sources and is accessible via a web service.
*  A web service is an API you can call via Hypertext Transfer Protocol (HTTP), the language of the World Wide Web.
* Web Scraping - If data exists in a structured format, you can retrieve it programmatically. Programmatic retrieval of data from a website is
* Human-in-the-Loop - the data you seek exists only in people's minds. 
* Surveys - urveys consist of one question and indicate customer satisfaction
* Survey Tools - Qualtrics is a powerful tool for developing and administering surveys.
* Observation - Observation is the act of collecting primary source data, from either people or machines.
*  Observational data can be qualitative or quantitative.
*   Collecting qualitative observational data leads to unstructured data challenges.
* Sampling - Once you have collected sample data, you can use statistical methods to make generalizations about the entire population

# Working With Data
* To turn a database design into an operational database ready to accept data, use the Data Definition Language (DDL) components of SQL.
*  DDL lets you create, modify, and delete tables and other associated database objects.
*  To generate insights, a productive analyst must be comfortable using the Data Manipulation Language (DML) capabilities of SQL to insert, modify, and retrieve information from databases.
*   While DDL manages the structure of a database, DML manages the data in the database.

*   Data Manipulation

*   Create new data
*   Read existing data
*   Update existing data
*   Delete existing data.

*   Create - INSERT - Creates new data in an existing table
*   Read - SELECT - Retrieves data from an existing table
*   Update - UPDATE- Changes existing data in an existing table
*   Delete- DELETE- Removes existing data from an existing table

*  SQL Considerations
* The keywords in SQL are case-insensitive.
* SQL can also span multiple lines.
* How a query appears is a function of organizational conventions. F
* actors that influence convention include database configuration, query efficiency, and how easy it is for people to read and understand the query.

# Filtering
* Filtering is a way to reduce the data down to only the rows that you need.
* To filter data, you add a WHERE clause to a query

* Filtering and Logical Operators
* A query can have multiple filtering conditions. use a logical operator to account for complex filtering needs.
* Complex queries frequently use multiple logical operators at the same time.
*  It is good to use parentheses around filter conditions to help make queries easy for people to read and understand.
*  Data warehouses often contain millions, billions, or even trillions of individual data records.
*  Filtering data is essential to making effective use of these massive data stores.

# Sorting
* When querying a database, you frequently specify the order in which you want your results to return.
* The ORDER BY clause is the component of a SQL query that makes sorting possible.
* do not have to specify the columns you are using to sort the data in the SELECT clause.
* The ASC keyword at the end of the ORDER BY clause sorts in ascending order whereas using DESC with ORDER BY sorts in descending order.
*  If you are sorting on multiple columns, you can use both ascending and descending as appropriate.
*   Both the ASC and DESC keywords work across various data types, including date, alphanumeric, and numeric.

# Date Functions
* date columns are frequently found in OLAP environments.
*  Date columns also appear in transactional systems. Storing date information about an event facilitates analysis across time.
*  The most important thing to note is that you have to understand the database platform you are using and how that platform handles dates and times
*  Since each platform provider uses different data types for handling this information, you need to familiarize yourself with the functions available from your provider of choice.

# Logical Functions
* Logical functions can make data substitutions when retrieving data.
* Remember that a SELECT statement only retrieves data.
* Another way to generate the output is by using the IFF logical function.
*  The IFF function has the following syntax: IFF(boolean_expression, true_value, false_value)
*  Boolean Expression:  The expression must return either TRUE or FALSE.
*  True Value:  If the Boolean expression returns TRUE, the IFF function will return this value.
*  False Value:  If the Boolean expression returns FALSE, the IFF function will return this value.
*  When using logical functions, you need to balance their convenience with the knowledge that you are replacing data from the database with the function's coded values.
*  The ability to do this type of substitution is a real asset when dividing data into categories.

# Aggregate Functions
* Summarized data helps answer questions that executives have, and aggregate functions are an easy way to summarize data.
* Aggregate functions summarize a query's data and return a single value.
* each database platform supports different aggregation functions
* COUNT -Returns the total number of rows of a query.
* MIN - Returns the minimum value from the results of a query. Note that this works on both alphanumeric and numeric data types.
* MAX - Returns the maximum value from the results of a query. Note that this works on both alphanumeric and numeric data types.
* AVG - Returns the mathematic average of the results of a query.
* SUM -Returns the sum of the results of a query.
* STDDEV- Returns the sample standard deviation of the results of a query.
* can also use aggregate functions to filter data. For example, you may want a query that shows all employees who make less than the average corporate salary.
*  Aggregate functions also operate across subsets of data. For instance, you can calculate total sales by month with a single query.

# System Functions
* Each database platform offers functions that expose data about the database itself.
*  One of the most frequently used system functions returns the current date.
*   The current date is a component of transactional records and enables time-based analysis in the future.
*    The current date is also necessary for a system that uses an effective date approach.
*    System functions also return data about the database environment. 

* Query Optimization
* Parametrization - Whenever a SQL query executes, the database has to parse the query. Parsing translates the human-readable SQL into code the database understands.
* Parsing takes time and impacts how long it takes for a query to return data. Effective use of parameterization reduces the number of times the database has to parse individual queries.
* Indexing -  A database index works like the index in the back of a book.
* Instead of looking at each page in a book to find what you are looking for, you can find a specific page number in the index and then go to that page.
A database index can point to a single column or multiple columns. When running queries on large tables, it is ideal if all of the columns you are retrieving exist in the index.
If that is not feasible, you at least want the first column in your SELECT statement to be covered by an index.
* While indexing improves query speed, it slows down create, update, and delete activity.
*  An indexing strategy needs to match the type of system the database supports, be it transactional or reporting.
*  Data Subsets and Temporary Tables - It is possible to create a temporary table to make the data more manageable.
*   Temporary tables can store the results of a query and are disposable. Temporary tables automatically get removed when the active session ends. Using temporary tables is an effective method of creating subsets for ad hoc analysis.
*   Execution Plan - An execution plan shows the details of how a database runs a specific query.
*   Execution plans are extremely helpful in troubleshooting query performance issues. They provide additional information about how a query is spending its time. For example, an execution plan can tell you if a slow-running query uses a full table scan instead of an index scan. 















